{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter website url: https://www.mycareersfuture.sg/search?search=research%20scientist&sortBy=new_posting_date&page=0\n",
      "Enter output file name: Research_Scientist_MCF\n"
     ]
    }
   ],
   "source": [
    "# URL, chromedriver path, minimum request & sleep interval\n",
    "url = input(\"Enter website url: \")\n",
    "\n",
    "output = input(\"Enter output file name: \")\n",
    "minreq = 5000\n",
    "timer = random.randint(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Scraping now!: https://www.mycareersfuture.sg/search?search=research%20scientist&sortBy=new_posting_date&page=0 ...wait for output file \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execution of URL in WebDriver and retrieve source\n",
    "chromedriver = \"chromedriver.exe\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(executable_path=chromedriver)\n",
    "driver.get(url.format(0))\n",
    "print(\"\\n\",\"Scraping now!: {} ...wait for output file\".format(url),\"\\n\")\n",
    "sleep(timer)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closes cookies warning - at the top\n",
    "button = driver.find_element_by_xpath('//*[@id=\"dismiss-button\"]')\n",
    "button.click()\n",
    "sleep(1)\n",
    "\n",
    "# Closes \"Add Skills\" pop-up - at the bottom\n",
    "button = driver.find_element_by_xpath('//*[@class=\"tr pointer OverlayNavigation__icon-cross___1wfSE\"]')\n",
    "button.click()\n",
    "sleep(1)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized all variables with empty lists\n",
    "job_id = []\n",
    "job_title = []\n",
    "company_name = []\n",
    "job_location = []\n",
    "job_level = []\n",
    "min_salary = []\n",
    "max_salary = []\n",
    "salary_type = []\n",
    "days_posted = []\n",
    "job_desc = []\n",
    "job_req = []\n",
    "job_type = []\n",
    "job_industry = []\n",
    "job_url = []\n",
    "job_count = 0\n",
    "jip_count = 0\n",
    "is_responsive = []\n",
    "is_contract = []\n",
    "is_intern = []\n",
    "is_temp = []\n",
    "is_perm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while job_count < minreq:\n",
    "    \n",
    "    list_of_jobs = soup.findAll(\"p\", {\"name\":\"company\"})\n",
    "    if jip_count < len(list_of_jobs):\n",
    "        \n",
    "        # Job ID\n",
    "        job_id_var = soup.findAll(\"div\", \"absolute top-0 right-0 JobCard__bookmark-box___pdZMT\")\n",
    "        for x in range(len(job_id_var)):\n",
    "            job_id.append(job_id_var[x].div.input[\"id\"])\n",
    "            \n",
    "        # Job Title\n",
    "        job_title_var = soup.findAll(\"h1\", {\"name\":\"job_title\"})\n",
    "        for x in range(len(job_title_var)):\n",
    "            job_title.append(job_title_var[x].text)\n",
    "            \n",
    "        # Company Name\n",
    "        company_name_var = soup.findAll(\"p\", {\"name\":\"company\"})\n",
    "        for x in range(len(company_name_var)):\n",
    "            company_name.append(company_name_var[x].text)            \n",
    "       \n",
    "        # Job Links\n",
    "        job_link_var = soup.findAll(\"a\", \"bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3\")\n",
    "        \n",
    "        for x in range(len(job_link_var)):\n",
    "            job_url.append(\"https://www.mycareersfuture.sg\"+job_link_var[x][\"href\"])            \n",
    "            \n",
    "            # Next Level Stuff\n",
    "            button = driver.find_element_by_xpath('//*[@href=\"'+job_link_var[x][\"href\"]+'\"]')\n",
    "            button.click()\n",
    "            sleep(timer)\n",
    "            soup_level2 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            \n",
    "            # Location\n",
    "            job_location_var = soup_level2.findAll(\"p\", {\"id\":\"address\"})\n",
    "            if len(job_location_var) > 0:\n",
    "                job_location.append(job_location_var[0].text)\n",
    "            else:\n",
    "                job_location.append(None)\n",
    "                \n",
    "            # Job level / Seniority (if any?)\n",
    "            job_level_var = soup_level2.findAll(\"p\", {\"id\": \"seniority\"})\n",
    "            if len(job_level_var) > 0:\n",
    "                job_level.append(job_level_var[0].text)\n",
    "            else:\n",
    "                job_level.append(\"Unknown\")\n",
    "                \n",
    "            # Salary\n",
    "            salary_var = soup_level2.findAll(\"span\", \"dib\")\n",
    "            if len(salary_var) > 0:\n",
    "                if len(salary_var[1].text.split(\"to\")) == 2:  # If there are 2 numbers given\n",
    "                    minimum = int(\"\".join(salary_var[1].text.split(\"to\")[0].split(\"$\")[1].split(\",\")))\n",
    "                    maximum = int(\"\".join(salary_var[1].text.split(\"to\")[1].split(\"$\")[1].split(\",\")))\n",
    "                    min_salary.append(minimum)\n",
    "                    max_salary.append(maximum)\n",
    "                    salary_type.append(salary_var[-2].text)\n",
    "                elif salary_var[1].text.split(\"to\")[0] == \"Salary undisclosed\": # Special Case (Non Recruiter)\n",
    "                    min_salary.append(None)\n",
    "                    max_salary.append(None)\n",
    "                    salary_type.append(None)\n",
    "                elif salary_var[2].text.split(\"to\")[0] == \"Salary undisclosed\": # Special Case (Recruiter)\n",
    "                    min_salary.append(None)\n",
    "                    max_salary.append(None)\n",
    "                    salary_type.append(None)\n",
    "                else:  # If there is only 1 number given\n",
    "                    minimum = int(\"\".join(salary_var[2].text.split(\"to\")[0].split(\"$\")[1].split(\",\")))\n",
    "                    maximum = int(\"\".join(salary_var[2].text.split(\"to\")[1].split(\"$\")[1].split(\",\")))\n",
    "                    min_salary.append(minimum)\n",
    "                    max_salary.append(maximum)\n",
    "                    salary_type.append(salary_var[-2].text)\n",
    "            else:  # If there are 0 number given\n",
    "                min_salary.append(0)\n",
    "                max_salary.append(0)\n",
    "                \n",
    "            # Job Type\n",
    "            job_type_var = soup_level2.findAll('p', {\"id\":\"employment_type\"})\n",
    "            list1 = [\"Contract\", \"Internship\", \"Temporary\", \"Permanent\"]\n",
    "            list2 = [is_contract, is_intern, is_temp, is_perm]\n",
    "            if len(job_type_var) > 0:\n",
    "                job_type.append(job_type_var[0].text)\n",
    "                for num, pos in enumerate(list1):\n",
    "                    if pos in job_type_var[0].text.split(\", \"):\n",
    "                        list2[num].append(1)\n",
    "                    else:\n",
    "                        list2[num].append(0)\n",
    "            else:\n",
    "                job_type.append(\"Unknown\")\n",
    "                for num, pos in enumerate(list1):\n",
    "                    list2[num].append(0)\n",
    "                    \n",
    "            # Job Industry\n",
    "            job_industry_var = soup_level2.findAll(\"p\", {\"id\": \"job-categories\"})\n",
    "            if len(job_industry_var) > 0:\n",
    "                job_industry.append(job_industry_var[0].text)\n",
    "            else:\n",
    "                job_industry.append(None)\n",
    "                \n",
    "            # Job Description\n",
    "            job_description_var = soup_level2.findAll(\"div\", {\"id\":\"description-content\"})\n",
    "            if job_description_var:\n",
    "                job_desc.append(job_description_var[0].text)\n",
    "            else:\n",
    "                job_desc.append(\"None\")\n",
    "                \n",
    "            # Job Requirements\n",
    "            job_requirements_var = soup_level2.findAll(\"div\", {\"id\":\"requirements-content\"})\n",
    "            if job_requirements_var:\n",
    "                job_req.append(job_requirements_var[0].text)\n",
    "            else:\n",
    "                job_req.append(\"None\")\n",
    "                \n",
    "            # Days Posted\n",
    "            days_posted_var = soup_level2.findAll(\"span\", {\"id\":\"last_posted_date\"})\n",
    "            if len(days_posted_var) > 0:\n",
    "                text = days_posted_var[0].text.split(\"Posted \")[1]\n",
    "                days_posted.append(text)\n",
    "            else:\n",
    "                days_posted.append(\"None\")\n",
    "                \n",
    "            driver.back()   \n",
    "            \n",
    "            job_count+=1\n",
    "            jip_count+=1\n",
    "            if job_count == minreq:\n",
    "                break\n",
    "    else:\n",
    "        if (soup.findAll(\"span\", {\"type\":\"action\"})[-1].text == \"‚ùØ\"):\n",
    "            button = driver.find_element_by_xpath('//*[@type=\"action\"][last()]')\n",
    "            button.click()\n",
    "            sleep(timer)\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            jip_count = 0\n",
    "            try:\n",
    "                # Dismiss Add Skills Popup\n",
    "                button = driver.find_element_by_xpath('//*[@class=\"joyride-tooltip__close\"]')\n",
    "                button.click()\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            driver.close()\n",
    "            print(\"Done\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame([job_id, job_url, job_title, job_type, company_name, job_location, job_level, job_industry, min_salary, \\\n",
    "                   max_salary, salary_type, days_posted, is_contract, is_intern, is_temp, is_perm, job_desc, \\\n",
    "                   job_req], index=[\"Job ID\", \"URL\", \"Job Title\", \"Job Type\", \"Company Name\", \"Job Location\", \"Job Level\", \\\n",
    "                                    \"Job Industry\", \"Min Salary\", \"Max Salary\", \"Salary Paid\", \"Date Posted\", \"Contract\", \\\n",
    "                                    \"Internship\", \"Temporary\", \"Permanent\", \"Job Description\", \"Job Requirements\"]).T\n",
    "\n",
    "# Drop any duplicate data row\n",
    "df.drop_duplicates(keep=False,inplace=True) \n",
    "\n",
    "# Exporting dataframe to CSV file\n",
    "df.to_csv(output+'.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
