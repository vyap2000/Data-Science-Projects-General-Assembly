,Job ID,URL,Job Title,Job Type,Company Name,Job Location,Job Level,Job Industry,Min Salary,Max Salary,Salary Paid,Date Posted,Contract,Internship,Temporary,Permanent,Job Description,Job Requirements
40,2a8b2d06e28c95bc8d5778e70e6a5b75,https://www.mycareersfuture.sg/job/data-engineer-intellect-minds-2a8b2d06e28c95bc8d5778e70e6a5b75,Data Engineer,Full Time,INTELLECT MINDS PTE. LTD.,"ANSON CENTRE, 51 ANSON ROAD 079904",Executive,Information Technology,5000,7000,Monthly,29 Jan 2019,0,0,0,0,"Company Overview Intellect Minds is a Singapore-based company since 2008, specializing in talent acquisition, application development, and training. We serve BIG MNCs and well-known clients in talent acquisition, application development, and training needs for Singapore, Malaysia, Brunei, Vietnam and Thailand. Our client is an establish company a, leader within their industry, is now looking for a Data Engineer to join their esteemed organization. Job Descriptions: Responsibilities • Create and maintain optimal data pipeline architecture. • Assemble large, complex data sets that meet functional / non-functional business requirements. • Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Cassandra, Hadoop and other Big Data Technologies. • Build data pipeline on premise and on Google Cloud Platform. • Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. • Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. • Work with data and analytics experts to strive for greater functionality in our data systems.","Qualifications • Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. • Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. • Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. • Strong analytic skills related to working with unstructured datasets. • Build processes supporting data transformation, data structures, metadata, dependency and workload management. • A successful history of manipulating, processing and extracting value from large disconnected datasets. • Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. • Experience supporting and working with cross-functional teams in a dynamic environment. • We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: • Experience with big data tools: Hadoop, Spark, Kafka, etc. • Experience with Google Cloud Platform esp. Google Pub-Sub, Big Query, Data Proc, Data Flow, Cloud Storage. • Experience with IoT & Time series data. • Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. • Experience with stream-processing systems: Storm, Spark-Streaming, etc. • Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. All successful candidates can expect a very competitive remuneration package and a comprehensive range of benefits. Interested Candidates please submit your detailed resume online. To your success! The Recruitment Team Intellect Minds Pte Ltd (Singapore)"
41,af05e08c9f5a51a65c5213bde728c60f,https://www.mycareersfuture.sg/job/high-performance-data-engineer-niometrics-af05e08c9f5a51a65c5213bde728c60f,High-Performance Data Engineer,Permanent,NIOMETRICS (PTE.) LTD.,"PARKVIEW SQUARE, 600 NORTH BRIDGE ROAD 188778",Professional,Information Technology,5500,11000,Monthly,29 Jan 2019,0,0,0,1,"WHAT WE DO We invite you to be part of our ambitious, close-knit team creating systems for large customers who need to crunch through Tbps of data in real-time. Our approach is relentless performance-oriented software engineering vs. server sprawl in our customers' datacentres. You will use the latest high-end hardware and continuously devise ways to push the envelope of software performance. We build in-house systems if we must. We had to for indexing 1M 60-column rows/s, for aggregating high throughput event streams over hundreds of combinations of dimensions, and for pattern matching 5M patterns at 100Gb/s per 2RU. We use these to solve real customer problems. You will experiment wildly. For example we implemented network monitoring using a GPU, and we tested 4-socket machines with 2T RAM. Our current favourite platform is a 2-socket system with E5-2699v4 CPUs (88 lcores in total), 4x40Gbps NICs and 1T RAM, which we use to process 160Gbps. You will help us build a successful software platform for the long run. We invest a lot in flexibility, such as with our extensible rule engine and declarative aggregation system that empowers our analysts and helps us minimise the C code we have to write for supporting disparate use-cases. We know the devil is in the details. You will improve performance through better memory allocation systems and better data structures, all while ensuring that they are integrated with Address Sanitizer and fully tested using unit tests and end-to-end regression tests. We work end-to-end. You will implement data engineering solutions that are both efficient and secure for handling events from 500 million users, and to extract insight without leaking individual information. We want to show off. To attract the best programmers we plan to showcase our technology. You can be part of our effort to open-source interesting pieces of our technology stack.   YOUR ROLE AS HIGH-PERFORMANCE DATA ENGINEER As a High-Performance Data Engineer, you will create and maintain tools, mainly in C, for crunching large amounts of data in files or streams. You will have to think both big, in terms of overall architecture, and small, in terms of low-level optimisations, to deliver solutions that are reusable, and match the performance of the best hardware. Every capability you add directly translates to new offerings made possible. Every percent of performance improvement directly translates to large cost savings. At the same time, the correctness and reliability of your work will be the cornerstone to our customers’ trust.",WHAT WE VALUE  Bachelor’s or Higher Degree in Computer Science or equivalent Software craftsmanship Attention to reliability and successful delivery Experience with large C code bases and high-performance C programming Familiarity with shared memory data structures and parallel algorithms Proficiency with Linux system & development tools 
42,2097b9ff0f6bb0197cb3b03afbbcb8b5,https://www.mycareersfuture.sg/job/lead-data-engineer-jewel-paymentech-2097b9ff0f6bb0197cb3b03afbbcb8b5,Lead Data Engineer,"Permanent, Full Time",JEWEL PAYMENTECH PTE. LTD.,"TECHLINK, 31 KAKI BUKIT ROAD 3 417818",Professional,Information Technology,8000,12000,Monthly,29 Jan 2019,0,0,0,1,"As a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you.  To be successful in this role, you will need to:  Capture/Analyse requirements and lead the design/architecture of solutions to meet requirements.  Write code by using best software development practices/security standards. Lead projects end-to-end from conceptualisation to deployment. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code.  Continuously improve knowledge on new technologies. Excellent in English, both written and spoken.","Required Qualifications:  10+ years of experience building highly scalable, low latency, fault tolerant systems. B.Sc., Masters, or equivalent experience in a quantitative field (Computer Science, Mathematics, Engineering, Artificial Intelligence, etc.). Hands on knowledge of at least 2 programming languages of Python/Java/Scala. In depth knowledge of at least 2 of Hadoop/Spark/Storm/Flink/Kafka. In depth knowledge of at least two NoSQL database (HBase/Cassandra/DynamoDB/Neo4j/Mongo/MemcacheDB) Good knowledge of at least some machine learning algorithms like logistic regression/ SVM/ Random Forests.  Knowledge of advanced data structures and algorithms. Preferred Qualifications: Knowledge of large scale ML systems like Tensorflow/pytorch. Knowledge of advanced machine learning algorithms like CNN/RNN Knowledge of Cloud environments like AWS/GCP. Knowledge of indexing systems like Elastic search/Solr/Lucene.  Proficient in using CI/CD and knowledge of Jenkins/SonarQube/Ansible. You’re a perfect fit us if you are   A master problem solver, and able to use own initiative to develop suitable solutions.  A strong communicator with the ability to convey information to others in a simple and unambiguous way.  An innovative, original thinker approach to job responsibilities, methods and processes.  An energetic person who can be trusted to get a job done.    "
43,72c619360a33276d981dbe73e9ca14f4,https://www.mycareersfuture.sg/job/data-scientist-application-production-engineer-keyteo-consulting-72c619360a33276d981dbe73e9ca14f4,Data scientist  /  Application production engineer,Full Time,KEYTEO CONSULTING PTE. LTD.,"HONG LEONG BUILDING, 16 RAFFLES QUAY 048581",Non-executive,Banking and Finance,8000,16000,Monthly,29 Jan 2019,0,0,0,0,"Our Company:     Founded in 2014, Keyteo Consulting is a company specialized in organization and information system management in financial and banking environments that work with its clients as they outsource their projects in innovation, as well as research and development. Our purpose is to improve the innovation, competitiveness and performances of our clients. We contribute to all the key steps in our clients’ project lifecycles, from an analysis of the needs through implementation and industrialization.  Keyteo Consulting offers strategic, operational and technological solutions intended to accompany clients as they carry out their projects, by providing complete expertise. Keyteo Consulting is strongly dedicated to sustain the strong growth of companies specializing in key sectors such as banking/finance and others. ","This role is a position within the eTrading Application Production team in Singapore that is part of the global APS e-Solutions group (Application Production & Support) representing eTrading and eCommerce stakeholders in the region. The team provides platform and infrastructure management for all front office Electronic Solutions systems, which consist of more than 50 Electronic trading and eCommerce applications hosted on approximately 500 Windows and Linux servers in Asia Pacific and 1000’s worldwide. The team also manages and supports Forex, Rates and Credit Direct Market Access (DMA) adaptors and their downstream dependencies. These adaptors connect to more than 20 electronic trading venues such as EBS, Reuters, JPX, ASX, SGX and Tradeweb as well as a number of external facing clients.  Team members require a strong understanding of eTrading systems using a broad range of skills such as Linux and Windows operating systems, externally connecting TCP/IP networks including multicast, firewalls, databases and eCommerce industry standard technologies such as load balancers and proxy servers. Team members are required to liaise with functional application support, development, IT infrastructure teams as well as business stakeholders while managing technical projects, resolving incidents, solving problems and fulfilling requests. The systems that the global team supports are based in in London, New York, Tokyo and Singapore and most of them are live 24/5.5, so close interaction with global counterparts occur on a daily basis Direct Responsibilities Ownership of Production systems:  Hands on technical support for the FX, Rates and Credit global trading platforms.  Work with counterparts in London, Mumbai and New York to provide a follow the sun  support model while taking responsibility for the management of incidents and resulting problems that occur in London and New York platforms during Asian hours.  Responsible for ensuring local monitoring, capacity / performance management and automation of local systems as a part of a global framework.  Plan, schedule, implement and document the change activities as well as troubleshooting incidents and resolving problems.  Work with autonomy and independence but within the framework of a global support structure. Contributing Responsibilities  Application integration, technical project management and implementation of the new requirements for applications and infrastructure that are within the Electronic Solutions Infrastructure scope. (ION Trading + eFX and Credit in-house systems)  Perform activities related to rollout of upgrades of existing services as well as coordinate and organize changes with external vendors, clients, markets and ECNs in Singapore, Japan and Australia.  Plan, schedule, implement and document the change activities as well as troubleshooting incidents and resolving problems.  Work closely with development teams, business analysts, architects and infrastructure teams to enhance existing platform. eg: resilience, BCP, system performance and low latency.  Support and management of the regional messaging infrastructure used by the eTrading platforms. (Tibco RV multicast, in-house systems) Technical & Behavioral Competencies  Sound understanding of both Windows and Linux server Operating Systems with a particular focus on application implementation and troubleshooting as well as performance tuning, low latency, time synchronisation and Operating System connectivity.  Good knowledge of TCP/IP networks with routing, multicast and multi-tiered firewall environments as there is a need to manage external connectivity to clients and exchanges via cross-connects and service providers.  Able to troubleshoot applications from a TCP and multicast level including packet captures, firewall and network routing analysis.  Ability to architect solutions that integrate trading applications into the existing environment while considering application failover and BCP, performance optimisation, latency and maximising the stability of the platform.  Solid scripting and automation skills required with a DevOps mindset. Python, shell scripting and Blade Logic experience a plus.  Experience in supporting and implementing a vendor based real time trading system such as ION trading.  Knowledge of FX and Rates Electronic Trading business functionality and trade flow. Specific Qualifications (if required)  Linux, Windows or Cisco certifications would be an advantage In addition to the skills and responsibilities mentioned in the JD, we’re looking for someone who has interest or exposure to data analytics. One of the project he will be in charge of may require data science on how to effectively gather and communicate upstream achievements of the team."
44,3a652fb39c0fb93f18173e29da6682b1,https://www.mycareersfuture.sg/job/data-engineer-woodpecker-asia-tech-3a652fb39c0fb93f18173e29da6682b1,Data Engineer,Permanent,WOODPECKER ASIA TECH PTE. LTD.,137 TELOK AYER STREET 068602,Professional,"Consulting , Banking and Finance, Information Technology",4800,5300,Monthly,28 Jan 2019,0,0,0,1,"About GoBear GoBear is Asia’s first transparent online personal finance marketplace. We don’t sell directly to our users – we’re the tool people use to find and compare banking and insurance products in 6 countries. Our free, easy-to-use platform offers the widest selection of products to make shopping for financial products transparent, easy and simple. We will help our users to have access to better products and higher approval rates. Through our data-driven technology we are empowering users and providers to build more meaningful relationships. GoBear’s strategy to evolve from a metasearch engine to an online marketplace will transform the Southeast Asia digital ecosystem for users and financial service providers. GoBear is one of the leading FinTech companies in the region and is featured frequently in the media and on stage.   The Job: Our Data Engineer (in the analytics team) is the person responsible for enhancing our analytics and performance management framework. You’ll be building our data infrastructure - like databases and large-scale data processing tools -  on the Google Cloud Platform.  You’ll be a perfect fit in this role if you’re an eager learner, have prior experience in quantitative domains, and if you’re keen to be a team player in a dynamic start-up.   You’ll get to:  Design, construct, install, test and maintain highly scalable data management systems Employ a variety scripting languages and tools to marry systems together Make sure our systems meet business requirements and industry practices Research opportunities for data acquisition and new uses for existing data Develop data set processes for data modelling, mining and production Integrate new data management technologies and software engineering tools into existing structures Create custom software components and analytics applications Install and update disaster recovery procedures Recommend ways to improve data reliability, efficiency and quality Collaborate with data architects, modelers and IT team members on project goals Build high-performance algorithms, prototypes, predictive models and proof of concepts ","Skills:  2-3 years of working experience as a Data Engineering or as a developer Bachelor’s or Master’s degree in Computer Science Proficient in Java and in one of the scripting languages such as Python, JavaScript, Ruby or PHP Proficient with No-SQL databases like HBase or MongoDB Proficient in ETL processes and programming models on Apache Beam Proficient in cloud computing systems management services such as Stackdriver Familiar with cloud services such as MS Azure, Google Cloud Platform or AWS Familiarity with Machine Learning and Statistical techniques for data mining will be beneficial Familiarity with Google Analytics and Google Tag Manager will be an advantage Proficient in oral and written communication in English, as well as effective interpersonal skills "
45,ab98de328007a20fc1f5ad9a4cd3e160,https://www.mycareersfuture.sg/job/data-engineer-jewel-paymentech-ab98de328007a20fc1f5ad9a4cd3e160,Data Engineer,"Permanent, Full Time",JEWEL PAYMENTECH PTE. LTD.,,Professional,Information Technology,4000,8000,Monthly,28 Jan 2019,0,0,0,1,"As a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you. To be successful in this role, you will need to:  Analyze requirements and deliver solutions that meet requirements. Write code by using best software development practices. Produce code that meets security standards. Estimate timelines and deliver solutions within agreed timeline. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code. Continuously improve knowledge on new technologies. Excellent in English, both written and spoken. "," B.Sc., Masters, or equivalent experience in a quantitative field (Computer Science, Mathematics, Engineering, Artificial Intelligence, etc.). Knowledge in the use and application of Python to develop complex software. General machine learning techniques and technologies (e.g., Bayesian classifiers, regression techniques, graphical models, working with unbalanced data-sets) as well as applications (e.g., predictive analytics). NoSQL Database Programming/Development. Manipulation of various types of data; data cleaning, filtering, and pre-processing for example with text/images. Knowledge and experience in the use of cloud computing platforms (AWS/Azure/GCP/etc). SQL familiarity and database technologies (e.g., row versus column stores, in-memory DB, DB clustering, HA for DB). Familiarity and experience with Linux environments. Understanding batch (e.g., Apache Hadoop / Map Reduce) and stream processing approaches / frameworks (e.g., Apache Spark).    You’re a perfect fit us if you are  A master problem solver, and able to use own initiative to develop suitable solutions. A strong communicator with the ability to convey information to others in a simple and unambiguous way. An innovative, original thinker approach to job responsibilities, methods and processes. An energetic person who can be trusted to get a job done. "
46,52a9c40c842ec8393282fdc31a1af874,https://www.mycareersfuture.sg/job/big-data-engineer-ernst-young-advisory-52a9c40c842ec8393282fdc31a1af874,Big Data Engineer (Financial Services),Full Time,ERNST & YOUNG ADVISORY PTE. LTD.,1 RAFFLES QUAY 048583,Middle Management,"Consulting , Banking and Finance, Information Technology",8000,16000,Monthly,28 Jan 2019,0,0,0,0,"We are the only professional services organisation who has a separate business dedicated exclusively to the financial services marketplace. Join Financial Services (FSO) and you will work with multi-disciplinary teams from around the world to deliver a global perspective. Aligned to key industry groups including asset management, banking and capital markets, insurance and private equity, we provide integrated advisory, assurance, tax, and transaction services.   The opportunity  As part of our Data and Analytics team of Financial Services Advisory practice you will work with multi-disciplinary teams to support clients in a wide range of big data initiatives aiming to generate and present new, useful and actionable insights. You will have the opportunity to work and take responsibilities in challenging engagements, gaining exposure to clients in various sectors both in Singapore and in the APAC region.    Your key responsibilities  Participation in large-scale client engagements. Contribution towards, or even leading, the delivery of innovative and engaging big data solutions. Understanding of business and technical requirements, provision of subject matter expertise, and implementation of big data engineering techniques. Conducting of data discovery activities, performing root cause analysis, and making recommendations for the remediation of data quality issues. Putting into practice good organizational and time management skills, with the ability to prioritize and complete multiple complex projects under tight deadlines. ","Skills and attributes for success  Leverage technology to continually learn, improve service delivery and maintain our leading edge best practices Strong presentation skills and proficiency in the use of PowerPoint, Word and Excel Good understanding of financial services industry    To qualify for the role you must have  Understanding or even practical experience of handling and manipulating semi-structured and unstructured data. Deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches available. Ability to deploy, manage, and administer Hadoop-based components. Ability to design, build, install, configure and support Hadoop-based applications. Experience with one of Java, C# or C++. Hands-on experience with HiveQL. Familiarity with data ingestion tools such as Kafka, Flume and Sqoop. Knowledge of hadoop related workflow/scheduling tools such as Oozie. Understanding of data modeling (ER models) techniques. Experience with investigating and handling data quality issues. Minimum 8 years hands-on experience in two (2) or more of the above areas. A Bachelor or Masters degree in Computer Science, Engineering, or other related fields.   Ideally, you’ll also have  Design and implementation experience of data models in physical form in one (or more) of the leading RDBMS platforms such as SQL Server, Oracle, IBM DB2/Netezza, Teradata, etc. Experience with Business Intelligence or statistical analysis tools and techniques. Strong communication and business relationship skills to effectively explain analysis, both verbally and in writing, to others and translate analysis into a clear business plan. Strong time management and organizational skills to gather and make use of data (both internal and external).   What we look for Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry. An effective communicator, you’ll be a confident team player that collaborates with people from various teams while looking to develop your career in a dynamic organization.   What working at EY offers We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. We also offer you:  Support, coaching and feedback from some of the most engaging colleagues around Opportunities to develop new skills and progress your career The freedom and flexibility to handle your role in a way that’s right for you    About EY As a global leader in assurance, tax, transaction and advisory services, we’re using the finance products, expertise and systems we’ve developed to build a better working world. That starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. Whenever you join, however long you stay, the exceptional EY experience lasts a lifetime. And with a commitment to hiring and developing the most passionate people, we’ll make our ambition to be the best employer by 2020 a reality.   If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible. Join us in building a better working world. Apply now.  "
47,5debf6c4120e2ba7be4bb2f83d24bf98,https://www.mycareersfuture.sg/job/data-centre-support-engineer-creative-infrastructure-solutions-5debf6c4120e2ba7be4bb2f83d24bf98,Data  Centre  Support  Engineer,Unknown,CREATIVE INFRASTRUCTURE SOLUTIONS PTE. LTD.,,Unknown,,0,0,Monthly,None,0,0,0,0,None,None
48,4e0d066faaec184fde6053448841ea60,https://www.mycareersfuture.sg/job/senior-engineer-engineer-data-centre-facilities-management-starhub-4e0d066faaec184fde6053448841ea60,"Senior Engineer /  Engineer, Data Centre Facilities Management","Permanent, Full Time",STARHUB LTD.,"STARHUB GREEN, 67 UBI AVENUE 1 408942",Fresh/entry level,Engineering,3500,4800,Monthly,28 Jan 2019,0,0,0,1,"Oversee security, access control, process streamlining, audits and certification of our data centres; assess utility needs, and other general facility management matters.   Responsibilities  Assist in day-to-day operation and maintenance of all the various facilities systems in StarHub Data Centres and Central Offices, involving systems such as Generators, Uninterruptible Power Supply (UPS), DC Rectifier System, Static Transfer Switch (STS), Fire Protection System, Building Management System, HT/LT electrical system, all ACMV systems, etc Ensure the routine maintenance of the facilities systems being carried out according to planned schedule and contracted scope of work Plan, call for and evaluate tender specifications; project manage any expansion projects to cater for users’ requirements Ensure enforcement of security at StarHub sites by managing physical security and security systems such as the card access, CCTV surveillance, visitor tracking, etc. Perform 24 x 7 standby for fault & maintenance activation Execute FM customers’ related work orders within/by the customers’ requested Ready for Service (RFS) Dates Update and maintain all records relating to M&E, security, and building/ schematic drawings of the facilities Conduct regular checks of the premises for potential safety hazards and unsafe conditions; initiate corrective measures to maintain a safe and healthy work environment, in compliance with prevailing legislative and regulatory requirements Conduct risk assessment and management ","Qualifications  Bachelor degree or Diploma in Engineering At least 2 years of relevant experiences, preferably in an M&E and DC environment  We regret that only shortlisted candidates will be notified."
49,d6b37d54d9b37d2ff9688ff0d05a7da6,https://www.mycareersfuture.sg/job/data-technical-support-engineer-talent-trader-group-d6b37d54d9b37d2ff9688ff0d05a7da6,Data Technical Support Engineer,"Permanent, Full Time",TALENT TRADER GROUP PTE. LTD.,"GATEWAY EAST, 152 BEACH ROAD 189721",Senior Executive,Information Technology,2500,4000,Monthly,28 Jan 2019,0,0,0,1," 1st level support on transmission, VoIP services and switching equipment  Manage ticket queue and responsibility for the issues Provide technical support through emails, tickets and phone Manage the network and systems for internal issues Responsible for support and maintenance for tracking, documentation and verification Arrange the roster shift for the team members "," Hands-on experience in network concepts Comfortable to work in Data Centre environment Independent, self-motivated and dynamic personality Experience in Data Centre NOC    Interested candidates who wish to apply for the advertised position, please email us an updated copy of your resume to; Email Address: it@talenttradergroup.com   EA License No.: 13C6305 Registration ID: R1333012   For candidate who applied for the advertised position is deemed to have consented to us that we may collect, use or disclosed your personal information for purpose in connection with the services provided by us.  "
50,165e3137bfd84a80aa511eb2574a2e44,https://www.mycareersfuture.sg/job/mct-big-data-engineer-165e3137bfd84a80aa511eb2574a2e44,MCT Big Data Engineer,"Permanent, Full Time",Company Undisclosed,,Non-executive,"Engineering, Manufacturing",3400,6800,Monthly,28 Jan 2019,0,0,0,1,"Do you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites","Qualifications and Experience: * B.S degree or M.S. degree with 2 years’ experience in Computer Engineering, Industrial Engineering, or any other discipline with extensive programming or machine learning work  * Minimum 2 years of experience working in big data and data science projects and teams * Extensive experience with Java, Scala, Python in Hadoop ecosystem (Spark, Hive, HBase, etc.) is a must * Extensive experience with at least one relational databases (MS SQL, Oracle, MySQL, Teradata, etc.) is a must * Experience with building analytical web applications and data visualization technologies (Django, Javascript, Bootstrap, D3, etc.) is a plus * Good grasp of statistical and scientific programming packages in Python, R, etc. * Good grasp of data science concepts with emphasis on machine learning techniques is a plus * Experience with image processing (OpenCV, Python PIL, scikit-image, etc.) is a plus * Proficiency with collaborative source code management and documentation tools. (GIT, JIRA, Confluence, etc.) * Strong communication skills (written, verbal and presentation) * Willing to do international travel"
51,d811457c351ebac1d6c04c22a4681965,https://www.mycareersfuture.sg/job/mct-big-data-senior-engineer-d811457c351ebac1d6c04c22a4681965,MCT Big Data Senior Engineer,"Permanent, Full Time",Company Undisclosed,,Non-executive,"Engineering, Manufacturing",5000,10000,Monthly,28 Jan 2019,0,0,0,1,"Do you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites","Qualifications and Experience: * B.S degree or M.S. degree with 2 years’ experience in Computer Engineering, Industrial Engineering, or any other discipline with extensive programming or machine learning work  * Minimum 2 years of experience working in big data and data science projects and teams * Extensive experience with Java, Scala, Python in Hadoop ecosystem (Spark, Hive, HBase, etc.) is a must * Extensive experience with at least one relational databases (MS SQL, Oracle, MySQL, Teradata, etc.) is a must * Experience with building analytical web applications and data visualization technologies (Django, Javascript, Bootstrap, D3, etc.) is a plus * Good grasp of statistical and scientific programming packages in Python, R, etc. * Good grasp of data science concepts with emphasis on machine learning techniques is a plus * Experience with image processing (OpenCV, Python PIL, scikit-image, etc.) is a plus * Proficiency with collaborative source code management and documentation tools. (GIT, JIRA, Confluence, etc.) * Strong communication skills (written, verbal and presentation) * Willing to do international travel"
52,4b1aeea089cbaa6726eb2cb5dca20fd0,https://www.mycareersfuture.sg/job/senior-etl-data-engineer-smartsoft-4b1aeea089cbaa6726eb2cb5dca20fd0,Senior ETL and DATA Engineer,Full Time,SMARTSOFT PTE. LTD.,"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",Senior Executive,Information Technology,6000,11000,Monthly,25 Jan 2019,0,0,0,0,"  Responsibilities include understanding ETL & Data Engineering requirements, architecture design, development of etl mapping and framework using SQL, Informatica, Python, Google Bigquery & Google Dataflow. This is a technical position providing hands-on delivery role, working with the cross-functional teams, while ensuring excellent cross functional relationship.   Job Details:  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Google ‘big data’ technologies. Build processes supporting data transformation, data structures, metadata, dependency and workload management. Work with stakeholders including the BSA, Report developers, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Additional responsibilities include troubleshooting, maintenance, and optimization or enhancement of existing processes. Partner with engineering leads and architects to define & coordinate technical design. Design and code reviews to ensure standards and quality level for the build Performance tuning of ETL jobs to meet SLA Prepare technical documentations on the deliverables Identify, define and implement best practices for process improvements for SDLC management ","Experiences:  Must have 4 to 6 years of experience using SQL, Informatica, Python & Bigquery. Must be hands-on and have working experience in SQL, Informatica 10.x, Python. Working experience with Google Bigquery, Google Dataflow & Exasol is a big plus. Working experience with Kafka or Google Pubsub is a big plus. Hands-on experience in development using best practices and standards on Informatica products specifically PowerCenter 9.x/10.x and Web Service Transformation Solid working skills preferably in Oracle RDBMS, SQL, PL/SQL and ODS/3NF db design considering performance and SLA. Strong design skills with a proven track record of success on large/highly complex projects preferably in the area of Enterprise Apps and Integration. Must have the ability to communicate technical issues and observations. Must have experience in cross functional domain and end to end knowledge of business and technology. Technical and functional knowledge in Oracle EBS & Siebel CRM are preferred.  Must possess excellent verbal and written communication skills. Must be able to effectively communicate & work with fellow team members and other functional team members to coordinate & meet deliverables"
53,31003e58ccc2f49ba4d79ddd8a696ca4,https://www.mycareersfuture.sg/job/senior-etl-data-engineer-schellden-global-services-31003e58ccc2f49ba4d79ddd8a696ca4,Senior ETL and Data Engineer,Full Time,SCHELLDEN GLOBAL SERVICES,"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",Senior Executive,Information Technology,6000,11000,Monthly,25 Jan 2019,0,0,0,0,"Responsibilities include understanding ETL & Data Engineering requirements, architecture design, development of etl mapping and framework using SQL, Informatica, Python, Google Bigquery & Google Dataflow. This is a technical position providing hands-on delivery role, working with the cross-functional teams, while ensuring excellent cross functional relationship.   Job Details:  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Google ‘big data’ technologies. Build processes supporting data transformation, data structures, metadata, dependency and workload management. Work with stakeholders including the BSA, Report developers, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Additional responsibilities include troubleshooting, maintenance, and optimization or enhancement of existing processes. Partner with engineering leads and architects to define & coordinate technical design. Design and code reviews to ensure standards and quality level for the build Performance tuning of ETL jobs to meet SLA Prepare technical documentations on the deliverables Identify, define and implement best practices for process improvements for SDLC management "," Must have 4 to 6 years of experience using SQL, Informatica, Python & Bigquery. Must be hands-on and have working experience in SQL, Informatica 10.x, Python. Working experience with Google Bigquery, Google Dataflow & Exasol is a big plus. Working experience with Kafka or Google Pubsub is a big plus. Hands-on experience in development using best practices and standards on Informatica products specifically PowerCenter 9.x/10.x and Web Service Transformation Solid working skills preferably in Oracle RDBMS, SQL, PL/SQL and ODS/3NF db design considering performance and SLA. Strong design skills with a proven track record of success on large/highly complex projects preferably in the area of Enterprise Apps and Integration. Must have the ability to communicate technical issues and observations. Must have experience in cross functional domain and end to end knowledge of business and technology. Technical and functional knowledge in Oracle EBS & Siebel CRM are preferred. Must possess excellent verbal and written communication skills. Must be able to effectively communicate & work with fellow team members and other functional team members to coordinate & meet deliverables. "
54,99f611ea6cf10ae6754dd32381943bcd,https://www.mycareersfuture.sg/job/senior-etl-data-engineer-schellden-global-99f611ea6cf10ae6754dd32381943bcd,Senior ETL and Data Engineer,Full Time,SCHELLDEN GLOBAL PTE. LTD.,"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",Senior Executive,Information Technology,6000,11000,Monthly,25 Jan 2019,0,0,0,0,"Responsibilities include understanding ETL & Data Engineering requirements, architecture design, development of etl mapping and framework using SQL, Informatica, Python, Google Bigquery & Google Dataflow. This is a technical position providing hands-on delivery role, working with the cross-functional teams, while ensuring excellent cross functional relationship.   Job Details:  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Google ‘big data’ technologies. Build processes supporting data transformation, data structures, metadata, dependency and workload management. Work with stakeholders including the BSA, Report developers, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Additional responsibilities include troubleshooting, maintenance, and optimization or enhancement of existing processes. Partner with engineering leads and architects to define & coordinate technical design. Design and code reviews to ensure standards and quality level for the build Performance tuning of ETL jobs to meet SLA Prepare technical documentations on the deliverables Identify, define and implement best practices for process improvements for SDLC management ","Experiences:  Must have 4 to 6 years of experience using SQL, Informatica, Python & Bigquery. Must be hands-on and have working experience in SQL, Informatica 10.x, Python. Working experience with Google Bigquery, Google Dataflow & Exasol is a big plus. Working experience with Kafka or Google Pubsub is a big plus. Hands-on experience in development using best practices and standards on Informatica products specifically PowerCenter 9.x/10.x and Web Service Transformation Solid working skills preferably in Oracle RDBMS, SQL, PL/SQL and ODS/3NF db design considering performance and SLA. Strong design skills with a proven track record of success on large/highly complex projects preferably in the area of Enterprise Apps and Integration. Must have the ability to communicate technical issues and observations. Must have experience in cross functional domain and end to end knowledge of business and technology. Technical and functional knowledge in Oracle EBS & Siebel CRM are preferred. Must possess excellent verbal and written communication skills. Must be able to effectively communicate & work with fellow team members and other functional team members to coordinate & meet deliverables. "
55,c94d217a77e02ce6753d0390cf9fdca7,https://www.mycareersfuture.sg/job/big-data-engineer-ernst-young-advisory-c94d217a77e02ce6753d0390cf9fdca7,Big Data Engineer (Financial Services),Full Time,ERNST & YOUNG ADVISORY PTE. LTD.,1 RAFFLES QUAY 048583,Manager,"Consulting , Banking and Finance, Information Technology",6000,12000,Monthly,25 Jan 2019,0,0,0,0,"We are the only professional services organisation who has a separate business dedicated exclusively to the financial services marketplace. Join Financial Services (FSO) and you will work with multi-disciplinary teams from around the world to deliver a global perspective. Aligned to key industry groups including asset management, banking and capital markets, insurance and private equity, we provide integrated advisory, assurance, tax, and transaction services.   The opportunity  As part of our Data and Analytics team of Financial Services Advisory practice you will work with multi-disciplinary teams to support clients in a wide range of big data initiatives aiming to generate and present new, useful and actionable insights. You will have the opportunity to work and take responsibilities in challenging engagements, gaining exposure to clients in various sectors both in Singapore and in the APAC region.    Your key responsibilities  Participation in large-scale client engagements. Contribution towards, or even leading, the delivery of innovative and engaging big data solutions. Understanding of business and technical requirements, provision of subject matter expertise, and implementation of big data engineering techniques. Conducting of data discovery activities, performing root cause analysis, and making recommendations for the remediation of data quality issues. Putting into practice good organizational and time management skills, with the ability to prioritize and complete multiple complex projects under tight deadlines. ","Skills and attributes for success  Leverage technology to continually learn, improve service delivery and maintain our leading edge best practices Strong presentation skills and proficiency in the use of PowerPoint, Word and Excel Good understanding of financial services industry    To qualify for the role you must have  Understanding or even practical experience of handling and manipulating semi-structured and unstructured data. Deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches available. Ability to deploy, manage, and administer Hadoop-based components. Ability to design, build, install, configure and support Hadoop-based applications. Experience with one of Java, C# or C++. Hands-on experience with HiveQL. Familiarity with data ingestion tools such as Kafka, Flume and Sqoop. Knowledge of hadoop related workflow/scheduling tools such as Oozie. Understanding of data modeling (ER models) techniques. Experience with investigating and handling data quality issues. Minimum 5 years hands-on experience in two (2) or more of the above areas. A Bachelor or Masters degree in Computer Science, Engineering, or other related fields.   Ideally, you’ll also have  Design and implementation experience of data models in physical form in one (or more) of the leading RDBMS platforms such as SQL Server, Oracle, IBM DB2/Netezza, Teradata, etc. Experience with Business Intelligence or statistical analysis tools and techniques. Strong communication and business relationship skills to effectively explain analysis, both verbally and in writing, to others and translate analysis into a clear business plan. Strong time management and organizational skills to gather and make use of data (both internal and external).   What we look for Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry. An effective communicator, you’ll be a confident team player that collaborates with people from various teams while looking to develop your career in a dynamic organization.   What working at EY offers We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. We also offer you:  Support, coaching and feedback from some of the most engaging colleagues around Opportunities to develop new skills and progress your career The freedom and flexibility to handle your role in a way that’s right for you    About EY As a global leader in assurance, tax, transaction and advisory services, we’re using the finance products, expertise and systems we’ve developed to build a better working world. That starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. Whenever you join, however long you stay, the exceptional EY experience lasts a lifetime. And with a commitment to hiring and developing the most passionate people, we’ll make our ambition to be the best employer by 2020 a reality.   If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible. Join us in building a better working world. Apply now.  "
56,0e254e8e63d01bd52efed3f91e4e1c0c,https://www.mycareersfuture.sg/job/big-data-engineer-ernst-young-advisory-0e254e8e63d01bd52efed3f91e4e1c0c,Big Data Engineer (Financial Services),Full Time,ERNST & YOUNG ADVISORY PTE. LTD.,1 RAFFLES QUAY 048583,Senior Executive,"Consulting , Banking and Finance, Information Technology",4100,8200,Monthly,25 Jan 2019,0,0,0,0,"We are the only professional services organisation who has a separate business dedicated exclusively to the financial services marketplace. Join Financial Services (FSO) and you will work with multi-disciplinary teams from around the world to deliver a global perspective. Aligned to key industry groups including asset management, banking and capital markets, insurance and private equity, we provide integrated advisory, assurance, tax, and transaction services.   The opportunity  As part of our Data and Analytics team of Financial Services Advisory practice you will work with multi-disciplinary teams to support clients in a wide range of big data initiatives aiming to generate and present new, useful and actionable insights. You will have the opportunity to work and take responsibilities in challenging engagements, gaining exposure to clients in various sectors both in Singapore and in the APAC region.    Your key responsibilities  Participation in large-scale client engagements. Contribution towards, or even leading, the delivery of innovative and engaging big data solutions. Understanding of business and technical requirements, provision of subject matter expertise, and implementation of big data engineering techniques. Conducting of data discovery activities, performing root cause analysis, and making recommendations for the remediation of data quality issues. Putting into practice good organizational and time management skills, with the ability to prioritize and complete multiple complex projects under tight deadlines. ","Skills and attributes for success  Leverage technology to continually learn, improve service delivery and maintain our leading edge best practices Strong presentation skills and proficiency in the use of PowerPoint, Word and Excel Good understanding of financial services industry    To qualify for the role you must have  Understanding or even practical experience of handling and manipulating semi-structured and unstructured data. Deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches available. Ability to deploy, manage, and administer Hadoop-based components. Ability to design, build, install, configure and support Hadoop-based applications. Experience with one of Java, C# or C++. Hands-on experience with HiveQL. Familiarity with data ingestion tools such as Kafka, Flume and Sqoop. Knowledge of hadoop related workflow/scheduling tools such as Oozie. Understanding of data modeling (ER models) techniques. Experience with investigating and handling data quality issues. Minimum 3 years hands-on experience in two (2) or more of the above areas. A Bachelor or Masters degree in Computer Science, Engineering, or other related fields.   Ideally, you’ll also have  Design and implementation experience of data models in physical form in one (or more) of the leading RDBMS platforms such as SQL Server, Oracle, IBM DB2/Netezza, Teradata, etc. Experience with Business Intelligence or statistical analysis tools and techniques. Strong communication and business relationship skills to effectively explain analysis, both verbally and in writing, to others and translate analysis into a clear business plan. Strong time management and organizational skills to gather and make use of data (both internal and external).   What we look for Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry. An effective communicator, you’ll be a confident team player that collaborates with people from various teams while looking to develop your career in a dynamic organization.   What working at EY offers We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. We also offer you:  Support, coaching and feedback from some of the most engaging colleagues around Opportunities to develop new skills and progress your career The freedom and flexibility to handle your role in a way that’s right for you    About EY As a global leader in assurance, tax, transaction and advisory services, we’re using the finance products, expertise and systems we’ve developed to build a better working world. That starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. Whenever you join, however long you stay, the exceptional EY experience lasts a lifetime. And with a commitment to hiring and developing the most passionate people, we’ll make our ambition to be the best employer by 2020 a reality.   If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible. Join us in building a better working world. Apply now.  "
57,5c6015c43915d53b5bec72fef296a1f6,https://www.mycareersfuture.sg/job/big-data-engineer-ernst-young-advisory-5c6015c43915d53b5bec72fef296a1f6,Big Data Engineer (Financial Services),Full Time,ERNST & YOUNG ADVISORY PTE. LTD.,1 RAFFLES QUAY 048583,Manager,"Consulting , Banking and Finance, Information Technology",6000,12000,Monthly,25 Jan 2019,0,0,0,0,"We are the only professional services organisation who has a separate business dedicated exclusively to the financial services marketplace. Join Financial Services (FSO) and you will work with multi-disciplinary teams from around the world to deliver a global perspective. Aligned to key industry groups including asset management, banking and capital markets, insurance and private equity, we provide integrated advisory, assurance, tax, and transaction services.   The opportunity  As part of our Data and Analytics team of Financial Services Advisory practice you will work with multi-disciplinary teams to support clients in a wide range of big data initiatives aiming to generate and present new, useful and actionable insights. You will have the opportunity to work and take responsibilities in challenging engagements, gaining exposure to clients in various sectors both in Singapore and in the APAC region.    Your key responsibilities  Participation in large-scale client engagements. Contribution towards, or even leading, the delivery of innovative and engaging big data solutions. Understanding of business and technical requirements, provision of subject matter expertise, and implementation of big data engineering techniques. Conducting of data discovery activities, performing root cause analysis, and making recommendations for the remediation of data quality issues. Putting into practice good organizational and time management skills, with the ability to prioritize and complete multiple complex projects under tight deadlines. ","Skills and attributes for success  Leverage technology to continually learn, improve service delivery and maintain our leading edge best practices Strong presentation skills and proficiency in the use of PowerPoint, Word and Excel Good understanding of financial services industry    To qualify for the role you must have  Understanding or even practical experience of handling and manipulating semi-structured and unstructured data. Deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches available. Ability to deploy, manage, and administer Hadoop-based components. Ability to design, build, install, configure and support Hadoop-based applications. Experience with one of Java, C# or C++. Hands-on experience with HiveQL. Familiarity with data ingestion tools such as Kafka, Flume and Sqoop. Knowledge of hadoop related workflow/scheduling tools such as Oozie. Understanding of data modeling (ER models) techniques. Experience with investigating and handling data quality issues. Minimum 5 years hands-on experience in two (2) or more of the above areas. A Bachelor or Masters degree in Computer Science, Engineering, or other related fields.   Ideally, you’ll also have  Design and implementation experience of data models in physical form in one (or more) of the leading RDBMS platforms such as SQL Server, Oracle, IBM DB2/Netezza, Teradata, etc. Experience with Business Intelligence or statistical analysis tools and techniques. Strong communication and business relationship skills to effectively explain analysis, both verbally and in writing, to others and translate analysis into a clear business plan. Strong time management and organizational skills to gather and make use of data (both internal and external).   What we look for Highly motivated individuals with excellent problem-solving skills and the ability to prioritize shifting workloads in a rapidly changing industry. An effective communicator, you’ll be a confident team player that collaborates with people from various teams while looking to develop your career in a dynamic organization.   What working at EY offers We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. We also offer you:  Support, coaching and feedback from some of the most engaging colleagues around Opportunities to develop new skills and progress your career The freedom and flexibility to handle your role in a way that’s right for you    About EY As a global leader in assurance, tax, transaction and advisory services, we’re using the finance products, expertise and systems we’ve developed to build a better working world. That starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. Whenever you join, however long you stay, the exceptional EY experience lasts a lifetime. And with a commitment to hiring and developing the most passionate people, we’ll make our ambition to be the best employer by 2020 a reality.   If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible. Join us in building a better working world. Apply now.  "
58,5a343fc2c67aacdc5771b512595ef0af,https://www.mycareersfuture.sg/job/senior-data-center-engineer-itcan-5a343fc2c67aacdc5771b512595ef0af,Senior Data Center Engineer,Full Time,ITCAN PTE. LIMITED,"PRUDENTIAL TOWER, 30 CECIL STREET 049712",Senior Executive,Information Technology,4500,7000,Monthly,25 Jan 2019,0,0,0,0,"Title: Senior Data Center Engineer JD:   Provide infrastructure and security monitoring/support for a 24x7 data centre. Carry out service requests which include code deployment, restart servers - VM - backup - storage, manage batch job processing, perform security scanning, escort duty etc. The candidate will work with DC Provider Services team and their vendors to review and coordinate  tech room layout, backbone connectivity, and rack elevation drawings.  ",More than 3+ Years of exp Data Center VM - backup - storage  
59,f93dc298de35c9c833d992120476878c,https://www.mycareersfuture.sg/job/-big-data-engineer-f93dc298de35c9c833d992120476878c,IT - BIG DATA ENGINEER,"Permanent, Full Time",Company Undisclosed,,Non-executive,"Engineering, Manufacturing",3400,6800,Monthly,25 Jan 2019,0,0,0,1,"Responsibilities: Be part of a DevOps team that design, build and maintain innovative Smart Manufacturing solutions and Big Data platform.  Participate in Agile development lifecycle for software & solution related to Smart Manufacturing and Big Data platform.   Work with Data Science within company to develop, automate and maintain reliable data analytic and mining solutions for Smart Manufacturing and Big Data platform.  Ability to assess current IT environments and make recommendations to increase capacity needs.  Communicate, collaborate and coordinate on Smart Manufacturing and Big Data related activities to various level of stakeholders and senior management.","Requirements: Bachelor’s or Master’s degree Computer Science, Electrical & Electronics/Computer/Software Engineering, Information Systems or related fields.   Fresh graduates are welcome to apply and for those with good understanding and hands-on experience in the following areas will be advantageous.    Hadoop based technologies such as HDFS, MapReduce, Hive, MongoDB, HBase, Spark etc.  Data warehousing solutions and latest (NoSQL) database technologies.  Programming or scripting languages like Java, Linux, Matlab, C#/C++, Python, Perl and/or R on Linux/Windows platforms.  Big Data visualization and reporting software like Tableau.  ETL/BI solutions using Microsoft SSIS, Informatica or having DB programming experience (TSQL, PLSQL).    Effective oral and written communication with strong analytical, problem solving, multitasking and project management skills are essential on the job."
60,e4ebc7f9c89dcf28691bfa6a882f2795,https://www.mycareersfuture.sg/job/product-data-management-senior-engineer-wanco-manpower-e4ebc7f9c89dcf28691bfa6a882f2795,IT Product Data Management Senior Engineer,Permanent,WANCO MANPOWER PTE LTD,"NORTHSTAR @ AMK, 7030 ANG MO KIO AVENUE 5 569880",Executive,Information Technology,3500,5300,,24 Jan 2019,0,0,0,1," The person will join our Dynamic Data team to lead the Data projects to support our factory This will require leadership, autonomy and strong technical skills The data group objective is to ensure the data quality and to be the key person in the organization to understand the overall data models The job is based in our Semiconductor factory in (Singapore) and requires close work with people located in our headquarters in France Configuration of new products (BOM, Routings etc) in our MES Siview and our ERP Oracle eBusiness suite Management of Engineering Change Notes (evolutions in the configurations) Help users to define the requirements, validation of the demands, implementation Reference for application of the Data Acquisition corporate rules Documentation of the changes and of the operational procedures Development of relevant tooling to optimize the activities Work closely with other local team: IT CIM (automation), process, production, finance etc... Strong interaction with Data Acquisition team located in France "," Bachelor’s degree in engineering, IT or related field required. (M) 3 to 5 years of related experience, prior experience in the semiconductor industry desired.(M) Technical skills: DB2, SQL.(M) Proficiency with MES system Siview.(plus) Working level communication skills in French, desired.(plus) Demonstrated leadership skills and coordinate on-site and remote teams Highly organized, practical, analytical and detail oriented Ability to multi-task, prioritize tasks, make critical decisions. Ability to work independently as well as in a team environment. Ability to work productively with frequent interruptions. Flexible schedule and able to work the hours required to meet deadlines. "
61,e0d13bb6e16d7168d605c8c511db3ed3,https://www.mycareersfuture.sg/job/research-engineer-e0d13bb6e16d7168d605c8c511db3ed3,Research Engineer (Process Industry /  Data Analytics),Permanent,Company Undisclosed,,Senior Executive,"Consulting , Engineering",,,Monthly,24 Jan 2019,0,0,0,1," Plan, co-ordinate and conduct co-innovation workshops. Lead as Req mgr for the co-innovation program Work closely with researchers to investigate the business challenges, innovate solutions and convert them to business opportunities. Find business value of innovations. Work closely with industrial partners to implement and test the solution prototypes Align & work closely with Dept management to convert prototypes and PoC’s to business proposals to HQ stakeholders Participate and present in User conferences and other seminars to promote the solution concepts. Support marketing team to sell the solution concepts at customer sites, worldwide. ", Atleast 3~5yrs operations experience in  chemical industry Participated in major turnarounds and worked in the capacity of coordinating major revamp /debottlenecking projects Keen on advanced analytics and has experience in demonstrating these skills Energy Management / Heat integration / Process Optimization Advanced Data Analytics Matlab or R knowledge is an added advantage 
62,1b585050532bdd76ad9421adc07b5a92,https://www.mycareersfuture.sg/job/python-etl-big-data-engineer-cxa-group-1b585050532bdd76ad9421adc07b5a92,Python ETL & Big Data Engineer,Full Time,CXA GROUP PTE. LIMITED,"HAW PAR TECHNO CENTRE, 401 COMMONWEALTH DRIVE 149598",Professional,Information Technology,6000,8000,Monthly,23 Jan 2019,0,0,0,0,"In this role, you would be responsible for:  Creating and maintaining robust ETL jobs Ensuring easy access to the data for the various data consumers Providing solutions for different storage and processing-related issues Contributing to the design of the company’s Data Architecture ","Requirements  Strong Python skills (including Pandas, SQLAlchemy) Proven ability to work efficiently with databases: SQL / NoSQL Experience with Hadoop / Spark is a strong plus Knowledge in Machine Learning - Optional Experience with Web and Front-end tools - Optional Strong software culture is very recommended Ability to express ideas clearly in writing and verbally University graduate with Engineering Degree Native Speaker / Full Fluency English "
63,60c011eb6b7af24a6e1597fd7d15b727,https://www.mycareersfuture.sg/job/data-engineer-collabera-technologies-60c011eb6b7af24a6e1597fd7d15b727,Data Engineer,Contract,COLLABERA TECHNOLOGIES PTE. LTD.,"PAYA LEBAR SQUARE, 60 PAYA LEBAR ROAD 409051",Senior Executive,Information Technology,5500,7150,Monthly,23 Jan 2019,1,0,0,0,"Ø  Possess a degree in Computer Science or related fields Ø  At least 8 year of working experience in data engineering on any relevant database technology - Experience in ETL tools, development in Microsoft SQL Server and Microsoft SQL Server Analysis Services (or any similar SQL/OLAP technology) Ø  Good communication skills, able to work independently with minimal supervision Ø  Good team player as this role will be part of a bigger teamØ ","Ø  Good understanding of data modeling concepts Ø  Experience in Python or Microsoft SQL Server Analysis Services is a plus Ø  Strong understanding of data modeling concepts and good ability to design various components of data model and data engineering solution Able to guide junior team members, review solution design and perform code review"
64,a8ee9115e5ac0b43cc6e5d451a14bd3f,https://www.mycareersfuture.sg/job/data-centre-engineer-helius-technologies-a8ee9115e5ac0b43cc6e5d451a14bd3f,Data Centre Engineer,Permanent,HELIUS TECHNOLOGIES PTE. LTD.,"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",Non-executive,Information Technology,3500,4000,Monthly,23 Jan 2019,0,0,0,1,"Job Objective Support mission critical data centres and remote buildings operations in Singapore. This roles is an 12-hour shift duty and support the data centre operations in data centres.   Job Description  Possess good knowledge of data centre operation tasks and duties. Perform day-to- day data centre / computer operations duties (key management, escorting vendors, facilities infrastructure checks, degaussing, routine checks, desktop & laptop management) Strong ability to support activities in data centre and computer rooms Ensure data centre physical security procedures are followed strictly. Prompt escalation of  incidents following the standard incident response procedure & track till closure To generate reports to management ","IDesired Skills, Knowledge and Experience  Minimum 1 year of Data Centre operations experience preferable in a team. Experience working in a high-pressured environment with 24x7 on-call responsibilities Strong understanding of incident, problem and change management procedures based on ITIL best practices Experienced in supporting Data Centre vendors, out-sourcing vendors. DCIM knowledge and usage (CA DCIM preferred) Ability to multi-task and proficient in Microsoft Office applications; Excel & PowerPoint Good written and verbal communication skills Highly motivated and self-driven An analytical and inquiring mind to derive innovative solutions. Ability to work independently and in a team Must have experience working in data centre operations in financial institutions.    Certification  Minimum diploma holder in computer science or Engineering Certified Data Centre Facilities Operation manager or  Equivalent Qualifications in ITIL Foundation level at a minimum   "
65,8ac4098c795dce10e7a48030a42255b3,https://www.mycareersfuture.sg/job/data-engineer-titansoft-8ac4098c795dce10e7a48030a42255b3,Data Engineer,Permanent,TITANSOFT PTE. LTD.,90 EU TONG SEN STREET 059811,Executive,Professional Services,3000,8000,,22 Jan 2019,0,0,0,1,"If you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate managers of data. Others see meaningless figures but they see value. Our team are part of mathematician, part of computer scientist, and part of interpreters. Of data. What a Data Engineer does in Titansoft Manage data warehouse with plans for a business vertical or a group of business verticals Generate and manage all allocated data sets including ensuring its quality based on requirements Work with our Data Infrastructure team to triage and resolve infrastructure issues Manage the delivery of high impact dashboards and data visualisation diagrams","What we are looking for in a Data Engineer Qualifications BA/BS in Computer Science, Electronics or Electrical Engineering, Information Technology or other qualified achievement Experience Hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Understand data architecture, data modeling and schema design What makes a (Super!) Data Engineer in Titansoft Patience to clean up huge amounts of data Passion to research domain knowledge Experience with large data sets, Hadoop, and data visualisation tools Interest in cloud computing and service (GCP, AWS, Azure) Interest in AI/ Machine-Learning product"
66,bff289fec02aff0092e988d84800d315,https://www.mycareersfuture.sg/job/big-data-engineer-devops-engineer-software-developer-add-on-apac-singapore-bff289fec02aff0092e988d84800d315,Big Data Engineer  /  DevOps Engineer  /  Software Developer,Permanent,ADD-ON APAC SINGAPORE PTE. LTD.,"ONE PEMIMPIN, 1 PEMIMPIN DRIVE 576151",Executive,Information Technology,,,Monthly,22 Jan 2019,0,0,0,1," Big Data Engineer  Responsibilities  Data sources integration and manipulation Design and build complex reports and dashboards       DevOps Engineer   Responsibilities  Responsible for deployment of advanced services on the cloud and on prem Implementation of automation solutions     Software Developer  Responsibilities  Assist in creating a tailor-made architecture Deployment of advanced services on the cloud Implement automation solutions Debug, troubleshoot and fix technical issues     "," Big Data Engineer  Job Requirements   3 years proven experience with big data tools preferably Splunk and/or ELK Proven scripting abilities in at least one of the following: Bash, Python, Ruby, Perl, PowerShell. Hands on Experience in building complex reports and dashboards based on big data platforms Hands on experience as cyber analyst or with SIEM tools– advantage Knowledge of cyber-attack techniques, threat vectors, risk management, and incident management Experience with implantation of cloud based big data systems and data analysis services  – advantage Experience with data visualization and charting (yFiles / d3.js / Chartist.js or similar) - advantage Excellent communication (writing and verbal)  skills in English, other languages - advantage Candidate must possess at least Bachelor's Degree/Post Graduate Diploma/Professional Degree in Engineering (Computer/Telecommunication) or equivalent.     DevOps Engineer   Job Requirements   3 years proven experience as a Linux/Windows Administrator  3 years proven experience with implementing end to end cloud based complex services     Design Implementation and optimization (technical and financial) Operation and monitoring   Hands on experience with big data solutions Hands on experience with cyber security technologies Very good understanding of networking, Linux and Windows administration. Experience with at least two of the following: Amazon Web Services, Microsoft Azure, Google Cloud Platform, VMware. Scripting abilities in at least one of the following: Bash, Python, Ruby, Perl, PowerShell. Creative troubleshooting skills and out-of-the-box thinking Excellent communication (writing and verbal) skills in English     Software Developer  Job Requirements  2+ years experience as full stack developer and web application development. Strong knowledge and experience with HTML5, CSS3, JavaScript(ES6) Experience with frontend frameworks. Proven scripting abilities in at least one of the following: Bash, Python, Ruby, Perl, PowerShell. Hand on experience with big data solutions preferably Splunk and/or ELK - advantage Hand on experience with cyber security technologies - advantage Experience with data visualization and charting (yFiles / d3.js / Chartist.js or similar) - advantage Graphic design orientation, passionate about clean design Experience with CSS preprocessors – advantage. Excellent communication (writing and verbal) skills in English       "
67,a88c75d9dd18a1cb9b7d92fc713a8337,https://www.mycareersfuture.sg/job/contract-senior-data-engineer-%E2%80%93-python-group-finance-dbs-bank-a88c75d9dd18a1cb9b7d92fc713a8337,"Contract Senior (1 Year Contract), Data Engineer – Python, Group Finance (190000E3)","Contract, Full Time",DBS BANK LTD.,,Junior Executive,Banking and Finance,2100,4200,Monthly,22 Jan 2019,1,0,0,0," Primary owner of cost allocation prototype model for Singapore and other countries Understand and translate cost allocation logic requirements to design and develop python scripts Enhance and modify existing python scripts to suit business needs Analyse all input data details to ensure that the data being sourced is complete and accurate Manage and drive script optimization, data storage and data validation as required    Key Responsibilities  Fit-for-purpose working prototype model to simulate the impact of the new Cost Allocation methodology for Singapore and other countries "," At least 2 years of experience using Python programming for data analysis preferably in a banking environment Degree in Information Systems, Computer Science, Engineering or related field with relevant experience in Python Strong presentation, analytical and problem-solving skills High attention to details, highly organized and able to work under pressure in a time-critical and fast-paced environment Results focused self-starter with ability to work with ambiguity and ‘can-do’ attitude Highly proficient in data manipulation using Python and Python libraries such as NumPy, Pandas Proven ability in manipulating and presenting large datasets using Business Intelligence (BI) tools including Microsoft Excel Experience with data visualization tools e.g. Tableau, Qlikview Work with various stakeholders from across the bank to understand logic and data requirements 	  "
68,8cea5272c7993d0c6addc977942ecead,https://www.mycareersfuture.sg/job/big-data-software-engineer-adecco-personnel-8cea5272c7993d0c6addc977942ecead,Big Data Software Engineer,"Contract, Full Time",ADECCO PERSONNEL PTE LTD,"SHAW CENTRE, 1 SCOTTS ROAD 228208","Professional, Executive","Banking and Finance, Information Technology",6000,9000,Monthly,22 Jan 2019,1,0,0,0,"The department is responsible for development and maintenance of Risk and Finance applications used by worldwide users covering Market Risk, Counterparty Risk, Finance domain. The applications are in-house developments with a mix of Microsoft and open source technologies.  The open position is to join one major investment project to tackle the regulatory requirement by redesigning information system platform to be global and adaptable enabling automated reporting and real-time processing and monitoring. The project will transform application landscape and bring it to the next level.  Main Responsibilities:  Lead technical study into a propose solution, while involving expertise from infrastructure big data expert, business analyst requirement Document proposed design and develop the solution Implicitly ensure all CI-CD artefacts are part of the solution Perform code review while fostering knowledge and coaching best practices to team members Interact and provide reporting to project managers Monitor technical risk and escalate appropriately to management  The position requires autonomy and reliability in performing duties with initiatives and leadership when it comes to all non-functional deliverables such as testing tools, mocking objects, production monitoring concerns, quality control including performance and load testing."," At least 7 years in Software development At least 6 years in Java/J2EE development At least 4 years experience in streaming solution Hands on Data ingest and data processing technology like Spark streaming and Spark Hands on Messaging systems like Kafka, Flume or ActiveMQ, MQSeries or RabitMQ Hands on knowledge on Hadoop (preferably Hortonworks distribution) - HDFS, HBase, Hive, ORC/Parquet Build tool - Maven/sbt/ant, UML, Restful web services, Jenkins/Team City, Source management – SVN/GIT, TDD using Junit, Jira/QC  Good to have:  Solution design using proven patterns, awareness of anti-patterns, performance tuning, especially in streaming Knowledge of tools like Phoenix, ElasticSearch, Sqoop, StreamSets are good to have. Basic understanding of finance and investment banking  Other Professional Skills and Mind-set  Excellent written and verbal communication skills for both team mates and management Strong analytical and problem solving skills Proficient software development life cycle Appetite to follow technology trend and participate to communities  Prepare your resume in word document (please include your current salary package with full breakdown such as base, incentives, annual wage supplement etc.) and expected package with your notice period (Including leaves to offset) and email it to TechnicalStaffing@adecco.com All shortlisted candidates will be contacted. Wai Yun Wen EA License No: 91C2918 Personnel Registration Number: R1330726"
69,2b1e7cfe58b6a681298840d3f44af017,https://www.mycareersfuture.sg/job/avp-data-engineer-ntuc-link-2b1e7cfe58b6a681298840d3f44af017,"AVP, Data Engineer",Full Time,NTUC LINK PRIVATE LIMITED,,Manager,Information Technology,8000,12000,Monthly,21 Jan 2019,0,0,0,0,"The rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.   As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.   Responsibility:  Define the overall data engineering and ETL frameworks across each SE for the CAO office. Working closely with data scientists and business analysts map out data requirements and data roadmap that will drive the analytical underpinnings for each SE work. Lead a team of 5-7 data engineers, defining the ETL tool kit, build ETL frameworks, manage the governance and SLA for each ETL deployment for analytical teams. Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  Design, build, support and optimize new and existing data models and ETL processes. Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. Develop and manage the various dashboards for management decision and data visualizations. Define and manage SLA for all data processes and own data quality issues.   ","Preferred qualification and skills:   Advanced degree in computer science, computer engineering, or other technical fields. 6-10 years’ experience having developed data engineering capabilities for large and complex franchises. Strong data modeling, schema design and SQL development skills. ETL/ELT implementation and data integration. Modern open source data visualization tools, eg. D3js, superset, plotly, leaflet,etc.  Big data platform development (Hadoop/Hive/Hbase/Spark, etc.) REST/Web API development and management. Hands-on experience in any modern programming language (Python or Java preferred). Design pattern, 12-factor app principle and modern cloud architecture. Self-motivated and proactive, willing to learn new things. Good communication skills and strong team player. "
70,abcc422cbdff6cbee3e26851b6204764,https://www.mycareersfuture.sg/job/data-engineer-ntuc-link-abcc422cbdff6cbee3e26851b6204764,Data Engineer,Full Time,NTUC LINK PRIVATE LIMITED,,Professional,"Engineering, Information Technology",5000,10000,Monthly,21 Jan 2019,0,0,0,0,"The rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.    As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibility: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  ","Preferred qualification and skills:  ·       Advanced degree in computer science, computer engineering, or other technical fields. ·       6-10 years’ experience having developed data engineering capabilities for large and complex franchises. ·       Strong data modeling, schema design and SQL development skills  ·       ETL/ELT implementation and data integration ·       Modern open source data visualization tools, eg. D3js, superset, plotly, leaflet,etc.  ·       Big data platform development (Hadoop/Hive/Hbase/Spark, etc.) ·       REST/Web API development and management ·       Hands-on experience in any modern programming language (Python or Java preferred) ·       Design pattern, 12-factor app principle and modern cloud architecture ·       Self-motivated and proactive, willing to learn new things ·       Good communication skills and strong team player"
71,62750ba5cc51c95fadc3b2cc22769e45,https://www.mycareersfuture.sg/job/data-center-electrical-engineer-singapore-google-asia-pacific-62750ba5cc51c95fadc3b2cc22769e45,Data Center Electrical Engineer - Singapore,Full Time,GOOGLE ASIA PACIFIC PTE. LTD.,"MARINA BAY FINANCIAL CENTRE, 8 MARINA BOULEVARD 018981",Senior Executive,Engineering,10750,21500,Monthly,21 Jan 2019,0,0,0,0,"Company overview: Google is not a conventional company, and we don’t intend to become one. True, we share attributes with the world’s most successful organizations – a focus on innovation and smart business practices comes to mind – but even as we continue to grow, we’re committed to retaining a small-company feel. At Google, we know that every employee has something important to say, and that every employee is integral to our success. We provide individually-tailored compensation packages that can be comprised of competitive salary, bonus, and equity components, along with the opportunity to earn further financial bonuses and rewards. Googlers thrive in small, focused teams and high-energy environments, believe in the ability of technology to change the world, and are as passionate about their lives as they are about their work. For more information, visit www.google.com/careers. The area: Engineering & Operations Google is and always will be an engineering company. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on millions, if not billions, of users. At Google, engineers not only revolutionize search, they routinely work on massive scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From Google Ads to Chrome, Android to YouTube, Social to Local, Google engineers are changing the world one technological achievement after another. The role: Data Center Electrical Engineer - Singapore Our thirst for technology is a part of everything we do. The Data Center Engineering team takes the physical design of our data centers into the future. Our lab mirrors a research and development department -- cutting-edge strategies are born, tested and tested again. Along with a team of great minds, you take on complex topics like how we use power or how to run state-of-the-art, environmentally-friendly facilities. You're a visionary who optimizes for efficiencies and never stops seeking improvements -- even small changes that can make a huge impact. You generate ideas, communicate recommendations to senior-level executives and drive implementation alongside facilities technicians. Additional Role Description: As a Data Center Electrical Engineer, you'll be involved in the development of complex electrical infrastructure - from site assessments and concept design, to construction projects and major modification and upgrade of existing infrastructure. You will also provide and prepare documents, including statement of work (SOW), total cost of ownership (TCO) analysis, concept designs, drawing markups, budgets, schedules, factory test and acceptance documents, final startup/commissioning reports and review and acceptance of as-built and submittals. Responsibilities: - Collaborate with the core team to understand, develop and update the data center electrical designs, starting from basis of design (BOD) to issue for constructions (IFC) documents for new data center project build-outs and major infrastructure upgrades to all levels of testing and commissioning works. - Manage all power system challenges during concept design, detailed design, procurement, bidding, manufacturing, delivery and installation on site. Identify and resolve issues raised by the cross-functional teams and various external stakeholders. - Generate scope of work for the equipment portions of major infrastructure upgrades based on project needs. Work with internal teams to update and document the configuration as it evolves. - Interface closely with the Global Commodities team, manufacturers and various global and local Engineering teams in order to deliver the electrical solutions needed for each project. - Update and maintain the internal design specifications, drawings and standards in accordance with the latest configurations and including all 'lessons learned' in relation to all equipment.","Minimum qualifications: - Bachelor's degree in Electrical Engineering or equivalent practical experience. - 6 years of experience in a bid build environment for mission critical facilities. - Ability to travel domestically and internationally up to 30% of the time as required. Preferred qualifications: - Professional Engineering License. - 10 years of experience in bidding, designing, operating and commissioning of electrical distribution systems, from high voltage (HV) transformer to branch circuits. - Experience with power system analysis and engineering software packages. - Demonstrated knowledge of mechanical and control systems. - Master's degree in Electrical Engineering."
72,540c77673bfefe3bafdb880658498799,https://www.mycareersfuture.sg/job/big-data-security-engineer-addstones-sas-540c77673bfefe3bafdb880658498799,Big Data Security Engineer,Permanent,ADDSTONES SAS,120 ROBINSON ROAD 068913,Manager,Consulting ,5000,10000,Monthly,21 Jan 2019,0,0,0,1,"GFI is an international IT services company, currently employing about 18,000 people Worldwide. GFI provides its clients with innovative, long-lasting industrial solutions to leverage performance from their information systems. We design and runs industrial platforms tailored to the economic and human considerations of its clients. • Management Consulting | Digital Transformation | Innovation • Operating over 20 countries • 2017 revenue of over 1,2 billion USD 48 years of existence.  In order to support our forthcoming businesses and technological challenges, we seek innovative and agile people sharing our mind set.  We are now looking for a Big Data Security Engineer to join our team in Singapore.","Context: The Bank systems generate a significant amount of data. To realize and increase the benefits from this data, Big Data Technologies and Methodologies are becoming substantially more valuable to organizations across all industries. By leveraging emerging technology, new risks are introduced into the organization, which potentially requires additional controls, protective and detective, to secure the use of Big Data and their environments. For 2019, the Bank is looking to reduce the risks introduced by the adoption of the Big Data through the definition of controls & technologies and awareness trainings for the Big Data users (Analyst, Data Scientist, Data engineer, etc). Skills requirements:  Solid knowledge in Big Data architectures and technologies Good knowledge in the security technologies and modules in use for Big Data platforms Good knowledge in Information Technology Risk assessment methodologies Good Knowledge in the Data protection and governance for Big Data platform Good communication and advocacy skills, both verbal and written, with the ability to express complex technical issues in an easily understood manner. Ability to collaborate and communicate effectively and respectfully with both business-oriented executives and technology-oriented personnel in teams across the organization  Profile: - At least 4 years of experience in managing Big Data platform - Significant experience in designing secured Big Data platform"
73,6ae8ced77930ec68b3876e9127e84d5a,https://www.mycareersfuture.sg/job/data-center-operations-engineer-mcits-technologies-6ae8ced77930ec68b3876e9127e84d5a,Data Center Operations Engineer,"Contract, Full Time",MCITS TECHNOLOGIES PTE. LTD.,"TONG BUILDING, 302 ORCHARD ROAD 238862",Executive,Information Technology,3400,4000,Monthly,20 Jan 2019,1,0,0,0,"Managed Services Provide 1st and 2nd Level incident/problem management, including ticket logging, problem identification, troubleshooting, resolution and escalation within agreed Service Level Agreement (SLA). Perform health and status monitoring of Systems inlcuding WAN, LAN, Wireless LAN Network using centralise fault and performance monitoring tools. Provide proactive health and status monitoring of server/system, storage, database, IP Telephony, security devices and application using centralised fault and performance monitoring tools. Perform basic server administration tasks including but not limited to creation of user accounts or resetting of passwords. The Individuals must interact with multiple parties and coordinate the recovery of system or network with the relevant vendors or support. Managed and work with agreed Service Level Agreement (SLA) with customers. Participate in UAT to ensure that new equipment/job function(s) are adhered to defined standards and specification. Facility Management Ensure Data Centre is operationally 24x7. Responsible for physical security of the Data Centre as defined in the Corporate Security Policy and standards Ensure Data Centre air conditioning units, lights and temperature are functional and within normal operational standards. Work as part of a team providing coverage on a 7 days a week, 24 hours per day (24x7) basis including public holidays, on a 12 hour rotating shift basis.   Remote Hand Service Provide vendor escorting service when in StarHub secured facilities. Provide basic cabling support like loose cable verification, connecting pre-laid cable – patching, etc. Provide visual checks and verifications of equipment (power cycle, equipment indicators, inspection of equipment, hardware reset). Provide basic checks on environmental conditions in the Data Centre. Provide basic data media loading/unloading to Tape Library using an enterprise backup application. Provide hardware parts replacement of systems (interface card & installation, slotting/removal of blades/line cards, movement of equipment). Assist in the labelling of Tape Media. Keep inventories of the movement of tape media using Tape Management System (TMS). Batch Job Management Ensure Daily Batch jobs for Billing and Revenue Collection are executed correctly and on time using a Enterprise Batch Scheduling tool. Monitor the progress of jobs execution and escalation to Application Support when necessary. Provide 1st Level Incident Management/Troubleshooting and act as point of contact with Banks and Merchants to resolve issues Ensure batch processing is on schedule and monitor all system and batch job status. Respond to all user enquries regarding system functionalities or billing and payment processing.     Tape Backup and Management Ensure daily tape backup are executed as scheduled and completed successfully. Provide 1st Level Incident Management/troubleshooting and escalate to engineering support when necessary. Provide backup jobs status/progress monitoring.  ","Qualification and Skills Minimum Diploma, preferable in Information Technology discipline, or equivalent required. Data Center Certified Professional , Cisco Certified Network Administrator (CCNA) with minimum 3-5 years hands-on experience in DC operations and/or network administration preferred. Applicant with minimum of 2-3 years Data Centre experience and supporting multi-vendor environment desirable. Experience in automated batch scheduler and enterprise data backup application desirable. Must be detailed oriented, highly organised and able to multi-task in an efficient manner. Knowledge of Sun Solaris, HP UX, MS Windows and Microsoft Office mandatory. Familiair with hardware/software components and terminology. Experience in analysing hardware and software problems and making quick and accurate diagnosis. Minimum of 2 years experience in Managed Services and Smart-Hand Services advantageous. Must be willing to work in a rotating shift and additional hours to support the team as required, including public holidays. Strong ethics, integrity and genuine concern for the needs of others. Must be a team player. Experience in customer support and interaction with corporate customers. Ability to workin a team environment, and also able to work under pressure with minimum supervision. Good initiatives, proactive with good command of spoken and written English."
74,38ad00cacb9beddc5e5b4a206592fd8f,https://www.mycareersfuture.sg/job/senior-data-engineer-jewel-paymentech-38ad00cacb9beddc5e5b4a206592fd8f,Senior Data Engineer,"Permanent, Full Time",JEWEL PAYMENTECH PTE. LTD.,"TECHLINK, 31 KAKI BUKIT ROAD 3 417818",Professional,Information Technology,8000,10000,Monthly,19 Jan 2019,0,0,0,1,"As a Senior Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you.   To be successful in this role, you will need to: •               Analyze requirements and deliver solutions that meet requirements. •               Write code by using best software development practices. •               Produce code that meets security standards. •               Estimate timelines and deliver solutions within agreed timeline. •               Write clear & concise documentation for solutions/code. •               Contribute ideas within team to build better code. •               Continuously improve knowledge on new technologies. •               Excellent in English, both written and spoken.  ","Education & Experience which will be relevant •    Three or more years of relevant work experience. •     B.Sc., Masters, or equivalent experience in a quantitative field (Computer Science, Mathematics, Engineering, Artificial Intelligence, etc.). •    Knowledge in the use and application of Python to develop complex software. •    General machine learning techniques and technologies (e.g., Bayesian classifiers, regression techniques, graphical models, working with unbalanced data-sets) as well as applications (e.g., predictive analytics). •     NoSQL Database Programming/Development. •     Manipulation of various types of data; data cleaning, filtering, and pre-processing for example with text/images. •     Knowledge and experience in the use of cloud computing platforms (AWS/Azure/GCP/etc). •     SQL familiarity and database technologies (e.g., row versus column stores, in-memory DB, DB clustering, HA for DB). •     Familiarity and experience with Linux environments. •     Understanding batch (e.g., Apache Hadoop / Map Reduce) and stream processing approaches / frameworks (e.g., Apache Spark).   You’re a perfect fit us if you are •    A master problem solver, and able to use own initiative to develop suitable solutions. •    A strong communicator with the ability to convey information to others in a simple and unambiguous way. •    An innovative, original thinker approach to job responsibilities, methods and processes. •    An energetic person who can be trusted to get a job done.  "
75,51f97c3a7f16fb63685c1e2f26effbad,https://www.mycareersfuture.sg/job/data-engineer-capgemini-singapore-51f97c3a7f16fb63685c1e2f26effbad,Data Engineer,Permanent,CAPGEMINI SINGAPORE PTE. LTD.,6 BATTERY ROAD 049909,Professional,Engineering,5000,7500,Monthly,18 Jan 2019,0,0,0,1,"Activities: • Industrialize data integration, data cleansing, data analytics programs or data management processes. • Contribute to the design, development, testing, deployment, performance in production and maintenance of the data-centric software including APIs, cloud-based architectures, libraries, toolbox. • Liaise with the Data Scientists, Architects, software developers, and business experts to understand how data needs to be converted, loaded, processed and presented. • Help the Data Architect to create an overview of the Data Lineage (from data flows, data transformations inside applications). • Provide clear documentation of the business rules embedded in the systems and potentially manage or help solving Data Quality issues. • Adapt to local context and tools provided by AXA’s entity as well as local entity development standards and IT landscape. Skills:  • Technical: Strong development skills - languages will depends on the entity but mainly python and Scala, Big Data experience (Spark, Hadoop Suite) Experience in Git, continuous integration and delivery. • Awareness on Data Management practices, including Data Lifecycle Management across Core IT & Big Data ecosystems as well as Data Privacy & Security constraints. • Knowledge in Data Modelling (including third normal form, star schema, data vault modelling methods). • Focus on end user and customer centricity, Strong oral and written communication skills, Passion for learning new tools, languages and frameworks, Fast adaptation to changing requirements, Strong problem-solving skills, able to work with minimal direct guidance, self-motivated and proactive, in a collaborative model, side by side with the business, Practical, hands on approach to get results. • Experienced in Cloud services & architecture. • Discipline in writing technical & non-technical documentation.","Activities: • Industrialize data integration, data cleansing, data analytics programs or data management processes. • Contribute to the design, development, testing, deployment, performance in production and maintenance of the data-centric software including APIs, cloud-based architectures, libraries, toolbox. • Liaise with the Data Scientists, Architects, software developers, and business experts to understand how data needs to be converted, loaded, processed and presented. • Help the Data Architect to create an overview of the Data Lineage (from data flows, data transformations inside applications). • Provide clear documentation of the business rules embedded in the systems and potentially manage or help solving Data Quality issues. • Adapt to local context and tools provided by AXA’s entity as well as local entity development standards and IT landscape. Skills:  • Technical: Strong development skills - languages will depends on the entity but mainly python and Scala, Big Data experience (Spark, Hadoop Suite) Experience in Git, continuous integration and delivery. • Awareness on Data Management practices, including Data Lifecycle Management across Core IT & Big Data ecosystems as well as Data Privacy & Security constraints. • Knowledge in Data Modelling (including third normal form, star schema, data vault modelling methods). • Focus on end user and customer centricity, Strong oral and written communication skills, Passion for learning new tools, languages and frameworks, Fast adaptation to changing requirements, Strong problem-solving skills, able to work with minimal direct guidance, self-motivated and proactive, in a collaborative model, side by side with the business, Practical, hands on approach to get results. • Experienced in Cloud services & architecture. • Discipline in writing technical & non-technical documentation."
76,abf461f251534f876db9a2d23175cb6e,https://www.mycareersfuture.sg/job/data-engineer-grabtaxi-holdings-abf461f251534f876db9a2d23175cb6e,Data Engineer,"Permanent, Full Time",GRABTAXI HOLDINGS PTE. LTD.,"OUE DOWNTOWN, 6 SHENTON WAY 068809",Professional,Information Technology,5300,8000,Monthly,18 Jan 2019,0,0,0,1," Build, deploy and manage big data solutions that can adequately handle the needs of a rapidly growing data driven company  Spearhead the development of systems, architectures, and platforms that can scale to the 3 Vs of Big data (Volume, Velocity, Variety)   Streamline data access and security to enable data scientists and analysts to easily access to data whenever they need to   Build out scalable and reliable ETL pipelines and processes to ingest data from a large number and variety of data sources   Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making     Lead the movement cleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientists   Run Modern high performance analytical databases and computation engines like RedShift, BigQuery, Greenplum,Presto and others  ","  A degree or higher in Computer Science, Electronics or Electrical Engineering, Software Engineering, Information Technology or other related technical disciplines.   Experience in handling large data sets (multiple TBs) and working with structured, unstructured and geographical datasets   Designed high performance scalable infrastructure stacks for Big Data Analytics   Deep understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline   Real passion for data, new data technologies, and discovering new and interesting solutions to the company’s data needs   Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis  "
77,5c0cbba35308ed1b1cefe4e70801f488,https://www.mycareersfuture.sg/job/data-engineer-facebook-singapore-5c0cbba35308ed1b1cefe4e70801f488,Data Engineer,Full Time,FACEBOOK SINGAPORE PTE. LTD.,"MARINA ONE WEST TOWER, 9 STRAITS VIEW 018937",Professional,Engineering,7500,12000,,18 Jan 2019,0,0,0,0,"Are you passionate about data? Do you like working with big data? Do you want to use data to help drive the direction of products that impact the lives of over 800 million people every day? If yes, we want to talk to you! Data and associated analytics play a huge role in the success of Facebook. We have diverse business needs and very large-scale data. This makes it a wonderful and exciting challenge to provide scalable, actionable, reliable and timely data for our company.   In this role, you’ll see a direct link between your work, company growth, and user satisfaction. You’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with (internal) stakeholders/teams and building scalable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.   This is a full time position based in our office in Singapore.","Responsibilties:   Manage data warehouse plans for a business vertical or a group of business verticals;  Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions;  Build data expertise and own data quality for allocated areas of ownership;  Design, build, optimize, launch and support new and existing data models and ETL processes in production;  Conduct design and code reviews;  Define and manage SLA for all data sets in allocated areas of ownership;  Work with data infrastructure to triage infra issues and drive to resolution;  Manage the delivery of high impact dashboards and data visualizations;  Minimum Qualifications:  BS/B.Tech./M.Tech in Computer Science, Math or related field;  2+ years hands-on experience in the data warehouse space, custom ETL design, implementation and maintenance;  2+ years hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred);  Strong data architecture, data modeling, schema design and effective project management skills;  Excellent communication skills and proven experience in leading data driven projects from definition through interpretation and execution;  Experience with large data sets, Hadoop, and data visualization tools  Ability to initiate and drive projects, and communicate data warehouse plans to internal clients/stakeholders "
78,a330651fbfd55e72e33da59c6fff3b38,https://www.mycareersfuture.sg/job/data-engineer-honestbee-a330651fbfd55e72e33da59c6fff3b38,Data Engineer,Permanent,HONESTBEE PTE. LTD.,"DELTA HOUSE, 2 ALEXANDRA ROAD 159919","Executive, Senior Executive",Information Technology,,,Monthly,18 Jan 2019,0,0,0,1,"As a Data Engineer at honestbee, you will be working alongside the brightest minds in the industry, solving some of the most challenging business problems using terabyte-scale of behavioural and transactional data. The solutions you create have a direct impact on millions of users in eight markets. What you'll be doing: - Be part of Asia’s strongest technology team in one of the world’s most exciting startups - Own, architect and scale honestbee’s bleeding-edge data platform  - Manage our data warehouse, OLAP, ETL systems, and data pipelines  - Contribute to further develop our best practices and innovation initiatives - Grow your skills and career in an environment that values continuous learning and personal development","Who you are: - Experienced in Python development - Proficient in SQL, Shell Scripting or another languages - Experience working with cloud-based and distributed systems (E.g. AWS ecosystem, Google Cloud, etc..) - Experience with building real-time stream-processing systems (E.g.Kafka, Fluentd, etc) - Knowledge of NoSQL databases (E.g.Redis, Elasticsearch, etc) - 2 to 5+ years of experience with data modelling and designing/supporting Data Warehouses (E.g. Redshift, BigQuery, etc) - 1 to 3+ years of experience in data processing/ETL implementation - Familiar with gunicorn, nginx, deployment and scalability (E.g. Kubernetes, Mesos, Marathon, etc)"
79,443ad7cb21097edb976947a1c864d995,https://www.mycareersfuture.sg/job/senior-data-engineer-thoughtworks-443ad7cb21097edb976947a1c864d995,Senior Data Engineer,Full Time,THOUGHTWORKS PTE. LTD.,"CHINA SQUARE CENTRAL, 18 CROSS STREET 048423",Professional,"Consulting , Information Technology",6000,11800,Monthly,18 Jan 2019,0,0,0,0,"Singapore, SingaporeThoughtWorks Singapore is looking for talented engineers passionate about building large scale data processing systems to help manage the ever-growing information needs of our clients.    You will be responsible for -   Creating complex data processing pipelines, as part of diverse, high energy teams Designing scalable implementations of the models  Hands-on programming based on TDD, usually in a pair programming environment Deploying data pipelines in production based on Continuous Delivery practices Advising clients on the usage of different distributed storage and computing technologies from the plethora of options available in the ecosystem ","Ideally, you should have -   5+ years of experience building and deploying large scale data processing pipelines in a production environment Production-level hands-on experience working on HDFS, Java MapReduce, Hive, Apache Spark, Oozie etc. Solid understanding of YARN, Mesos, MPP Databases, SQL-on-Hadoop solutions like Impala etc. Experience working with, or an interest in Agile Methodologies, such as Extreme Programming (XP) and Scrum Knowledge of software best practices, like Test-Driven Development (TDD) and Continuous Integration (CI) Strong communication and client-facing skills with the ability to work in a consulting environment is essential Senior developers (7+ years) are expected to be the Architect for small and large enterprise projects. On larger projects, you are expected to work closely with the fellow architects to come up with the architecture and take it further. Desire to contribute to the wider technical community through collaboration, coaching, and mentoring of other technologists  If you relish the idea of being part of a community that extends beyond the work we do for our customers, you may find ThoughtWorks is the right place for you. If you share our passion for technology and want to help change the world with software, we want to hear from you! To apply, please submit your CV and tell us why you want to join ThoughtWorks. We will ask you to write code as part of your interview process, so be prepared! Our recruiters will be in touch."
80,7175b74b3afe2f9160b03e9bd778f5e1,https://www.mycareersfuture.sg/job/data-center-support-engineer-servlink-technology-resources-7175b74b3afe2f9160b03e9bd778f5e1,Data Center Support Engineer,Permanent,SERVLINK TECHNOLOGY RESOURCES PTE LTD,"HENDERSON BUILDING, 221 HENDERSON ROAD 159557",Non-executive,Information Technology,2500,3200,Monthly,18 Jan 2019,0,0,0,1," Perform data daily walkthrough and update client shared database. Monitoring, update and closing of ticket via ticketing system with resolution summary. Using approved tools for incident and problem management, perform IMAC tasks when required. Support hardware replacement and Break-fix.  Restore on-site equipment back to working status. Media Management. Escort 3rd party vendor to all local data centers (during office hour & after office hour) Perform data center power up/down (PDU), preventive maintenance work on equipment. Manage onsite equipment and update relevant inventory systems. Compile relevant data and statistics for monthly reporting Other ad hoc data center activities as and when required. Other duties and job functions as may be instructed from time to time by the Company and may be transferred from one section to another at the sole discretion of the Company "," Min Diploma in IT or Computer Science or 1 - 2 years of relevant working experience  Required to work on permanent night shift (8pm - 8am), shift allowance will be given Working pattern (work 3 days, rest 3 days; work 2 days, rest 2 days) "
81,e2d23fa69f59f15f092b39e622c18c1d,https://www.mycareersfuture.sg/job/data-center-engineer-techcom-solutions-asia-pacific-e2d23fa69f59f15f092b39e622c18c1d,Data Center Engineer,"Contract, Full Time",TECHCOM SOLUTIONS ASIA PACIFIC PTE. LTD.,"SHENTON HOUSE, 3 SHENTON WAY 068805",Professional,Information Technology,5000,6500,Monthly,18 Jan 2019,1,0,0,0," 4+ years working experience in an Operations Centre (OCT), performing monitoring and issuing of commands to computers and IT equipment. Working experience of doing shift work would be highly preferred. OCT shall be on 12-hour shifts, performing day-shifts and night-shifts alternating with teammates in accordance to a shift pattern, with sufficient breaks. Alerts signalling abnormalities from various IT domains (e.g. applications, infrastructure, network, IT security) are centrally channelled to IOC. IOC is expected to monitor such alerts round-the-clock, and react accordingly, i.e. either escalate within SLA to the correct party who can resolve the issue, or execute well-defined instructions or procedures as prescribed or as instructed by domain experts. OCT is expected to execute well-defined routine operational instructions or procedures which are formulated by and handed over from various IT domains, in accordance to an approved daily schedule (which content might differ on different days and on different shifts). OCT is also expected to provide feedback on documents and update when assigned (periodically only), in order to keep them up-to-date. Able to read general IT instructions / procedures (in English) and execute accurately. [This part will be tested during interview session.]   "," 4+ years working experience in an Operations Centre (OCT), performing monitoring and issuing of commands to computers and IT equipment. Working experience of doing shift work would be highly preferred. OCT shall be on 12-hour shifts, performing day-shifts and night-shifts alternating with teammates in accordance to a shift pattern, with sufficient breaks. Alerts signalling abnormalities from various IT domains (e.g. applications, infrastructure, network, IT security) are centrally channelled to IOC. IOC is expected to monitor such alerts round-the-clock, and react accordingly, i.e. either escalate within SLA to the correct party who can resolve the issue, or execute well-defined instructions or procedures as prescribed or as instructed by domain experts. OCT is expected to execute well-defined routine operational instructions or procedures which are formulated by and handed over from various IT domains, in accordance to an approved daily schedule (which content might differ on different days and on different shifts). OCT is also expected to provide feedback on documents and update when assigned (periodically only), in order to keep them up-to-date. Able to read general IT instructions / procedures (in English) and execute accurately. [This part will be tested during interview session.]   "
82,cbbbf5fffe0ae23aedc84c16eadb476c,https://www.mycareersfuture.sg/job/data-engineer-nec-corporation-cbbbf5fffe0ae23aedc84c16eadb476c,Data Engineer,Full Time,NEC CORPORATION,"HYFLUX INNOVATION CENTRE, 80 BENDEMEER ROAD 339949",Executive,"Engineering, Information Technology",3000,4500,Monthly,17 Jan 2019,0,0,0,0,"The Innovation and Transformation Office (ITO) of Global Safety Division (GSD) leads disruptive idea generation, research, testing and prototyping with the goal of launching breakthrough products and services inspired by global trends and emerging technologies, contributing to a Safer City. We aim to take our deep technical and product expertise and develop innovative solutions that helps to transform businesses. This position is a good match for a passionate and technical leader who enjoys technical challenges and possess excellent communication skills. The successful candidate should preferably have a strong start-up mentality and experience, as well as proven track record within a cross-cultural multi-national corporation, delivering large mission critical solutions in the area of data science and machine learning. As a Data Engineer, you'll work closely with our customers to produce innovative and actionable quantitative models and analysis to address the challenges in the areas of Public Safety. The Innovation and Transformation Office (ITO) team is a catalysing force that crystalizes visionary concepts into proof-of-concepts (POCs) and prototypes that will bring real value to organizations. These solutions need to be scalable to support millions of customers worldwide.    Provide leadership in the areas of data analytics and modelling with a strong focus on Public Safety. Develop and deploy AI technologies for public safety. Manage machine learning projects, including writing functional and program specifications and documentation, data (structured and unstructured) acquisition from external and internal sources, data preparation (data cleaning, data mapping, data quantity and quality validation), identifying suitable machine learning algorithms to apply on the data sets, building machine learning model from the data, and tuning model parameters for enhanced performance. Develop processes and tools for evaluating the performance of machine learning algorithms and robustness of the models. Establish specific success criteria for selecting the best machine learning model to address a real-world problem. Perform system testing and user acceptance testing to validate the robustness and performance of the machine learning models. Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Design, build, test, validate, and deploy statistical and machine learning models to answer business needs and increase operational efficiency. Develop custom data models and algorithms to apply to data sets. Develop processes and tools to monitor and analyze model performance and data accuracy. Actively engage in the creation of new disruptive and transformational products and services through an understanding of the end user’s requirements and operating environment. Participate in the building and enhancement of a robust pipeline to support the automation of various development associated tasks to achieve continuous integration and delivery. Create Intellectual Property (IP) in the form of patents, publication of papers in the relevant areas that is tightly aligned to the strategic direction of the GSD. ","Education & Experience  Bachelor’s degree in Computer Science or Data Science (preferred). Or, Bachelor’s degree in any of Statistics/Mathematics/Technology plus a certification or rich experience in Machine Learning/Deep Learning/ETL. Minimum of 5 years of quantitative analytics experience with a focus on statistical modeling, Machine Learning, forecasting, optimization and/or predictive analytics. Experience in biometrics and facial recognition technologies is a plus.    Skill Requirements  At least 2 years’ relevant experience. Must have strong programming experience with Python and R. Experience in machine learning, deep learning, data visualization, statistical, text analytics libraries, jupyter notebook and/or frameworks in Python or R. Experience with data processing and data analytics. "
83,1d231290ad1b00aef2b0266e339cdcd4,https://www.mycareersfuture.sg/job/senior-database-consultant-big-data-engineer-palo-singapore-1d231290ad1b00aef2b0266e339cdcd4,Senior Database Consultant - Big Data Engineer,"Permanent, Full Time",PALO IT SINGAPORE PTE. LTD.,51B CIRCULAR ROAD 049406,Professional,Information Technology,6000,12000,Monthly,17 Jan 2019,0,0,0,1,"Your profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment","✔     You hold a Bachelor, Master or PhD degree in IT, Information Management and/or Computer Science ✔     You are just graduated or have less than 3 years of working experience ✔     Good knowledge of big data technology landscape and concepts related to distributed storage / computing ✔     Experience with big data frameworks (e.g. Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR) ✔     Experience with batch & ETL jobs to ingest and process data from multiple data sources ✔     Experience with NoSQL databases (e.g. Cassandra, MongoDB, Neo4J, ElasticSearch) ✔     Experience with querying tools (e.g Hive, Spark SQL, Impala) ✔     Experience or willingness to go in real-time stream processing, using solutions such as Kafka, Flume and/or Spark Streaming ✔     You are passionate about technology and continuous learning comes naturally to you  "
84,2d499a6a8a10936226fd9d712342007f,https://www.mycareersfuture.sg/job/senior-database-consultant-big-data-engineer-palo-singapore-2d499a6a8a10936226fd9d712342007f,Senior Database Consultant - Big Data Engineer,"Permanent, Full Time",PALO IT SINGAPORE PTE. LTD.,51B CIRCULAR ROAD 049406,Professional,Information Technology,6000,12000,Monthly,17 Jan 2019,0,0,0,1,"Your profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment","✔     You hold a Bachelor, Master or PhD degree in IT, Information Management and/or Computer Science ✔     You are just graduated or have less than 3 years of working experience ✔     Good knowledge of big data technology landscape and concepts related to distributed storage / computing ✔     Experience with big data frameworks (e.g. Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR) ✔     Experience with batch & ETL jobs to ingest and process data from multiple data sources ✔     Experience with NoSQL databases (e.g. Cassandra, MongoDB, Neo4J, ElasticSearch) ✔     Experience with querying tools (e.g Hive, Spark SQL, Impala) ✔     Experience or willingness to go in real-time stream processing, using solutions such as Kafka, Flume and/or Spark Streaming ✔     You are passionate about technology and continuous learning comes naturally to you  "
85,25cd0d5ffef2fd9be93c58763bc96425,https://www.mycareersfuture.sg/job/data-engineer-microsec-25cd0d5ffef2fd9be93c58763bc96425,Data Engineer,"Permanent, Full Time",MICROSEC PTE. LTD.,,Middle Management,Information Technology,4000,6000,Monthly,17 Jan 2019,0,0,0,1,"Responsibilities: 1. Developing, enhancing, automating, and managing analytics models for anomaly detection. 2. Run those models into production environment with static as well as distributed databases. 3. Exploring and evaluating new digital tools and techniques to improve the product’s operational capabilities 4. Provide engineering support of key testing activities, including support of laboratory and field testing activities Outcome: The candidate will get to work on the state of the art intrusion attacks and how to prevent them. As a startup company, the candidate will get to work on wide domain of technologies which will increase the breadth of the experience of the candidate. This will provide an insight information about the industry trend and will groom the candidate for future prospects.","Requirements: At least 2 year of experience in Python, Unix/Linux with machine learning tools including Tensorflow, LSTM and other models. Should have designed and built at least one system from ground up and made it into an application. Should also have experience in building APIs and using it for application integration along with Visualization tools using Python based production environment."
86,f1eba772347267532813c3a0de581003,https://www.mycareersfuture.sg/job/data-scientist-engineer-nec-corporation-f1eba772347267532813c3a0de581003,Data Scientist / Engineer,Full Time,NEC CORPORATION,"HYFLUX INNOVATION CENTRE, 80 BENDEMEER ROAD 339949",Executive,"Engineering, Information Technology",4500,6000,Monthly,17 Jan 2019,0,0,0,0,"The Innovation and Transformation Office (ITO) of Global Safety Division (GSD) leads disruptive idea generation, research, testing and prototyping with the goal of launching breakthrough products and services inspired by global trends and emerging technologies, contributing to a Safer City. We aim to take our deep technical and product expertise and develop innovative solutions that helps to transform businesses. This position is a good match for a passionate and technical leader who enjoys technical challenges and possess excellent communication skills. The successful candidate should preferably have a strong start-up mentality and experience, as well as proven track record within a cross-cultural multi-national corporation, delivering large mission critical solutions in the area of data science and machine learning. As a Data Scientist/Engineer, you'll work closely with our customers to produce innovative and actionable quantitative models and analysis to address the challenges in the areas of Public Safety. The Innovation and Transformation Office (ITO) team is a catalysing force that crystalizes visionary concepts into proof-of-concepts (POCs) and prototypes that will bring real value to organizations. These solutions need to be scalable to support millions of customers worldwide.    Provide leadership in the areas of data analytics and modelling with a strong focus on Public Safety. Develop and deploy AI technologies for public safety. Manage machine learning projects, including writing functional and program specifications and documentation, data (structured and unstructured) acquisition from external and internal sources, data preparation (data cleaning, data mapping, data quantity and quality validation), identifying suitable machine learning algorithms to apply on the data sets, building machine learning model from the data, and tuning model parameters for enhanced performance. Develop processes and tools for evaluating the performance of machine learning algorithms and robustness of the models. Establish specific success criteria for selecting the best machine learning model to address a real-world problem. Perform system testing and user acceptance testing to validate the robustness and performance of the machine learning models. Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Design, build, test, validate, and deploy statistical and machine learning models to answer business needs and increase operational efficiency. Develop custom data models and algorithms to apply to data sets. Develop processes and tools to monitor and analyze model performance and data accuracy. Actively engage in the creation of new disruptive and transformational products and services through an understanding of the end user’s requirements and operating environment. Participate in the building and enhancement of a robust pipeline to support the automation of various development associated tasks to achieve continuous integration and delivery. Create Intellectual Property (IP) in the form of patents, publication of papers in the relevant areas that is tightly aligned to the strategic direction of the GSD. ","Education & Experience  Bachelor’s degree in Computer Science or Data Science (preferred). Or, Bachelor’s degree in any of Statistics/Mathematics/Technology plus a certification or rich experience in Machine Learning/Deep Learning/ETL. Minimum of 5 years of quantitative analytics experience with a focus on statistical modeling, Machine Learning, forecasting, optimization and/or predictive analytics. Experience in biometrics and facial recognition technologies is a plus.    Skill Requirements  At least 2 years’ relevant experience. Must have strong programming experience with Python and R. Experience in machine learning, deep learning, data visualization, statistical, text analytics libraries, jupyter notebook and/or frameworks in Python or R. Experience with data processing and data analytics. "
87,47d32e8fabf96e2a4f09b02bc14e4ff2,https://www.mycareersfuture.sg/job/big-data-security-engineer-addstones-sas-47d32e8fabf96e2a4f09b02bc14e4ff2,Big Data Security Engineer,Permanent,ADDSTONES SAS,,Manager,"Consulting , Banking and Finance, Information Technology",7000,12000,Monthly,17 Jan 2019,0,0,0,1,"GFI Group is an international business and technology solutions provider, currently employing about 18,000 people Worldwide. GFI provides its clients with long-lasting innovative solutions to leverage performance from their information systems. We design and run industrial platforms tailored to the economic and human considerations of our clients. • Management Consulting | Digital Transformation | Innovation • Operating over 20 countries, • 2017 revenue of over 1,2 billion USD,  • 48 years of existence. In order to support our forthcoming businesses and technological challenges, we seek innovative and agile people sharing our mind set. We are now looking for a Big Data Security Engineer to join our team in Singapore.  Context: The Bank systems generate a significant amount of data. To realize and increase the benefits from this data, Big Data Technologies and Methodologies are becoming substantially more valuable to organizations across all industries. By leveraging emerging technology, new risks are introduced into the organization, which potentially requires additional controls, protective and detective, to secure the use of Big Data and their environments. For 2019, the Bank is looking to reduce the risks introduced by the adoption of the Big Data  through the definition of controls & technologies and awareness trainings for the Big Data users (Analyst, Data Scientist, Data engineer, etc). The main responsibilities of the Big Data Security Engineer are to identify the technologies to address the controls, implement Proof Of concepts and run comparative study for the tools.      ","  ·         Solid technical knowledge in Data discovery technologies and process ·         Solid technical knowledge in security technologies and modules in use for Big Data platforms ·         Good knowledge in Information Technology Risk assessment methodologies ·         Good Knowledge in the Data protection and governance for Big Data platform ·         Good communication and advocacy skills, both verbal and written, with the ability to express complex technical issues in an easily understood manner. ·         Ability to collaborate and communicate effectively and respectfully with both business-oriented executives and technology-oriented personnel in teams across the organization ·         At least 4 years of experience in implementing Security technologies for Big Data platform ·         Significant experience in running comparative study for security technologies and deployment of proof of concepts ·         Holder of information Security Certificate in Big Data technologies is preferable"
88,2a1269ac23a2fa6f8d993d86db70e47e,https://www.mycareersfuture.sg/job/senior-data-research-engineer-titansoft-2a1269ac23a2fa6f8d993d86db70e47e,Senior Data Research Engineer,Permanent,TITANSOFT PTE. LTD.,90 EU TONG SEN STREET 059811,Executive,Others,4000,8000,Monthly,17 Jan 2019,0,0,0,1,"Our research team at Titansoft focuses on Human Behaviour Imitation, Artificial Intelligence, and Probability Theory including, but not limited to, feedback control, algorithms, automatic processing, and machine learning models with the overall goal of building an automation system. We are looking for a Senior Data Research Engineer who shares a deep passion for machine learning programming and embrace the idea of teamwork. If you feel strongly about AI areas, then we definitely want to speak with you.  ","Qualifications Minimum Degree in Computer Science, Math, Physics, Engineering, Statistics or other technical fields. Experiences  Prior experience or course work in analytics and data mining, with focus on segmentation strategies and predictive models. Extensive programming experience with either Python, R, C# or other programming languages. Knowledge and experience in at least one of three of the following: Keras, Tensorflow and Pytorch. Demonstrated history of building prototypes for an AI project. Experience with AI / Machine Learning product or Kaggle Experience with multi-threaded design and parallel / distributed computing Working knowledge of C# and SQL  Skills  Systematic problem-solving approach, coupled with strong communication skills. Have strong algorithms/data structure knowledge Ability to understand complex systems Strong business sense and logical skills to balance data-driven decisions with intuition desire and capacity to learn, develop and lead the team. "
89,3d6111d96157c9a00507bd1fc9cf4796,https://www.mycareersfuture.sg/job/data-research-engineer-titansoft-3d6111d96157c9a00507bd1fc9cf4796,Data Research Engineer,Permanent,TITANSOFT PTE. LTD.,90 EU TONG SEN STREET 059811,Executive,Others,4000,8000,Monthly,17 Jan 2019,0,0,0,1,"Our research team at Titansoft focuses on Human Behaviour Imitation, Artificial Intelligence, and Probability Theory including, but not limited to, feedback control, algorithms, automatic processing, and machine learning models with the overall goal of building an automation system. We are looking for a Data Research Engineer who shares a deep passion for machine learning programming and embrace the idea of teamwork. If you feel strongly about AI areas, then we definitely want to speak with you.  "," Minimum Degree in Computer Science, Math, Physics, Engineering, Statistics or other technical fields. Knowledge and experience in at least one of these: Keras, Tensorflow or Pytorch. Strong knowledge and experience in Python. Strong knowledge of algorithm design.  To succeed in this role, it will be good to have:  Experience with an Artificial Intelligence or Machine Learning product or Kaggle. Knowledge in SQL or other programming languages. Contributions to open source projects. Familiarity with multi-threaded design and parallel or distributed computing. Working knowledge on C#. "
90,dcb2608aa088d095a1473874285d0a74,https://www.mycareersfuture.sg/job/technical-big-data-security-engineer-keyteo-consulting-dcb2608aa088d095a1473874285d0a74,Technical Big Data Security engineer,Full Time,KEYTEO CONSULTING PTE. LTD.,"HONG LEONG BUILDING, 16 RAFFLES QUAY 048581",Non-executive,Banking and Finance,6500,13000,Monthly,17 Jan 2019,0,0,0,0,"Our Company:  Founded in 2014, Keyteo Consulting is a company specialized in organization and information system management in financial and banking environments that work with its clients as they outsource their projects in innovation, as well as research and development. Our purpose is to improve the innovation, competitiveness and performances of our clients. We contribute to all the key steps in our clients’ project lifecycles, from an analysis of the needs through implementation and industrialization.  Keyteo Consulting offers strategic, operational and technological solutions intended to accompany clients as they carry out their projects, by providing complete expertise. Keyteo Consulting is strongly dedicated to sustain the strong growth of companies specializing in key sectors such as banking/finance and others.","Skills & Competencies requirements: i. Solid technical knowledge in Data discovery technologies and process ii. Solid technical knowledge in security technologies and modules in use for Big Data platforms iii. Good knowledge in Information Technology Risk assessment methodologies iv. Good Knowledge in the Data protection and governance for Big Data platform v. Good communication and advocacy skills, both verbal and written, with the ability to express complex technical issues in an easily understood manner. vi. Ability to collaborate and communicate effectively and respectfully with both business-oriented executives and technology-oriented personnel in teams across the organization Experience and Qualifications requirements: i. At least 4 years of experience in implementing Security technologies for Big Data platform ii. Significant experience in running comparative study for security technologies and deployment of proof of concepts iii. Holder of information Security Certificate in Big Data technologies is preferable"
91,8f859201827cc192dffeea3e4271b2b3,https://www.mycareersfuture.sg/job/data-platform-engineer-8f859201827cc192dffeea3e4271b2b3,Data Platform Engineer,Full Time,Company Undisclosed,,Senior Executive,Information Technology,4000,8000,Monthly,17 Jan 2019,0,0,0,0,"UCARE. AI'S team of data scientists and technologists came together with one mission: to use data ethically to solve real world problems and improve lives by creating the most advanced artifical intelligence capable of making accurate predictions years into the future. We are looking for 3 top-notch Python Hackers, who are well-versed in Python, hand-crafting our high performance data platform and AI engine (AlgoServer) and fine-tuning our proprietary algorithms (AlgoPacks) working side-by-side with our data scientists.  Your primary focus will be the on-going development and the enhancement of our data infrastructure, ingesting and analyzing ""big data"" across myriad of data sources, including our frontend mApp. You also will be working closely with senior data scientists to deliver proprietary algorithms running at scale across our data platform sitting on top of Spark. Duties and responsibilities:  Writing modular, scalable and efficient code jointly developed with senior tech team on agreed architecture. Implementation of security and data protection while ingesting and analyzing ""big data"" Real-time integration of various data sources, ingestion (ETL- extract, transform and load) of data through our data pipes, and execution of algo packs delivering insights back to client's systems. Liaising with frontend developers/vendors to support the backend data platform. Create and maintain software documentation. "," Preferred background in Computer Science, Computer Engineering or related disciplines. 1-4 years of relevant working experience, especially in large-scale production environment.  Fresh graduates with strong Python skills are encouraged to apply. Excellent prgramming skill in one or more scripting languages.  Python (must), Scala, Java, JavaScript. Understanding of the threading limitations of Python. and multi-process architecture. Background in machine learning is advantageous.  Additional experience in Node JS and React Native would be great (but not necessary). Experience in any one of the following field is highly preferred: (i)cloud computing platform (ii) database management (No SQL preferred) (iii) distributed computing/lambda architecture (iv) Sparks and HDFS Prior startup or entrepreneurial experience would be a bonus Can-do attitude "
92,e9ea9493274dd95bbe64877c715113cf,https://www.mycareersfuture.sg/job/data-engineer-alpha-z-analytics-singapore-e9ea9493274dd95bbe64877c715113cf,Data Engineer,Contract,ALPHA Z ANALYTICS SINGAPORE PTE. LTD.,,Professional,Professional Services,6000,10000,Annually,16 Jan 2019,1,0,0,0,"Responsibilities This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses.   The Data Engineer will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.  This person should have extensive experience in: ·       building data pipelines to pull together information from different source systems;  ·       optimizing the flow, collection, integration, consolidation and cleansing of data;  ·       structuring data for use in individual analytics application;  ·       data preparation and building data structures in relational and NoSQL environment/databases; ·       deploying / configuring analytic tools, optimizing computing environment for analytic projects; and ·       deploying analytic projects to production:  deploying data science product via either API or batch job, automating the refresh of machine learning models, deploying real time models.   The right candidate will be excited by the prospect of designing data engineering solutions from ground up. ·       Create and maintain optimal data pipeline architecture; ·       Assemble large, complex data sets that meet functional / non-functional business requirements; ·       Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, designing infrastructure for greater scalability, etc. ·       Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies; ·       Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; ·       Build processes supporting data transformation, data structures, dependency and workload management","·        Degree in computer science with at least 3 years data engineering work experience in big data analytics environment. ·        Excellent data engineering skills with open source big data stack ·        Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. ·        Strong analytic skills related to working with unstructured datasets.  ·        Experience with big data tools: Hadoop, Spark, Kafka, etc. ·        Experience with relational SQL and NoSQL databases ·        Experience with object-oriented/object function scripting languages: Python, Scala, etc. ·        Familiar with deployment and optimization of open source big data analytic stack on distributed environment. ·        Familiar with compiling, deploying and configuring open source data science tools including Python, R, Spark, etc. ·        Familiar with deploying analytic projects and data science products to production ·        Excellent programming skills"
93,a6afa6b4ba52dfdb024217fcb0b4eba0,https://www.mycareersfuture.sg/job/data-engineer-orica-international-a6afa6b4ba52dfdb024217fcb0b4eba0,Data Engineer,Permanent,ORICA INTERNATIONAL PTE. LTD.,78 SHENTON WAY 079120,Fresh/entry level,Information Technology,75000,90000,Monthly,16 Jan 2019,0,0,0,1,"Orica is seeking a bright and motivated individual to join our Data Science team. The candidate's primary responsibility will be to provide analytics services for the digital hub initiatives to help improve and monitor product reliability and operational efficiency. This will include developing insights from diverse data sources and creating platforms to make these insights actionable for key stakeholders.   The successful Data Engineer will propose creative Data Science solutions to problems faced by various groups at Orica, evaluate those solutions, and then work with the team to develop and deploy those solutions.   RESPONSIBILITIES    Perform large-scale data analysis and develop effective statistical models for segmentation, classification, optimization, time series, etc. Uses analytical platforms such as R and Python to model complex systems and derive actionable insights. Design and implement reporting dashboards that track key business metrics and provide actionable insights. Works with the team to review, analyse, and develop solutions for operational leaders. Work closely with both business units and engineering teams to formulate measurement problems and associated technical solution strategies Work closely with engineering and product management teams to build tools and applications on our unique big data platform to efficiently generate and deploy insights into decision-making systems at Orica. "," An advanced degree (Masters or PhD) in statistics, mathematics, computer science, engineering or scientific field Proficiency with statistical analysis tools to include: R, SAS, SPSS Proficiency with software development technologies to include: Python, C++, Java Knowledge of machine learning tools, basic statistics, data visualization techniques and databases (SQL) to Perform large-scale data analysis and develop effective statistical models for segmentation, classification, optimization, time series, etc. Excellent verbal and communication skills, ability to explain predictive analytics to non-technical audience Interface with Engineers, Product Managers and Product Analysts to understand product goals and data needs Build data expertise and own data quality for allocated areas of ownership Numerical skills with the ability to think logically and practically to deal with complexity. Ability to:   Work in a fast-paced environment Promptly recognize emerging problems and identify potential solutions Deliver high-quality results on time   "
94,d165fec9f2768af3e28feec4dfe6f4d3,https://www.mycareersfuture.sg/job/senior-data-engineer-dataspark-d165fec9f2768af3e28feec4dfe6f4d3,Senior Data Engineer,Permanent,DATASPARK PTE. LTD.,"COMCENTRE, 31 EXETER ROAD 239732","Executive, Senior Executive",Information Technology,6000,8000,Monthly,16 Jan 2019,0,0,0,1,"Responsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies ","Requirements  7+ years of superior software development experience building commercial large-scale software systems and database systems Excellence in algorithms, data structure, discrete math, data base and data warehousing Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills Experience of data warehouses in excess of 10TB Experience of Web UI, middle tier, and data back end development Production coding experience in choice of programming languages and development frameworks Proven professional experience in processing large-scale commercial data. Experience with telco data a plus. Superior and proactive communications skills, including verbal, written, and presentation. A proven team player and contributor. Self-directed, ability to work independently and research innovative solutions to business problems Aptitude of working on multiple projects in parallel Attention to details and data accuracy MS or BS degree in Computer Science/Engineering, Statistics, Mathematics, or equivalent is required for this position. "
95,6e7dd9f9380e41c9fc9fe4887a078696,https://www.mycareersfuture.sg/job/data-quality-engineer-alphatech-business-solutions-6e7dd9f9380e41c9fc9fe4887a078696,Data Quality Engineer,Permanent,ALPHATECH BUSINESS SOLUTIONS PTE. LTD.,,Senior Executive,Information Technology,6000,7800,Annually,16 Jan 2019,0,0,0,1, Evaluate and recommend solutions via data analysis regarding issues related to the improvement of product qua;oty and resolving of customer feedback Apply software and programming abilities to manage and analyse data from a variety of sources ,"   Must know JAVA8 and SPARK Experience in distributed data architecture Have working knowledge of SQL, Python, Airflow Scala, Hadoop, SPARK Good to know CI/CD Experience (Jenkins Github), AWS, Kubernetes, Docker Preferred to have banking domain experience "
96,3221d10d99d0b258bc2f476978aa8e9f,https://www.mycareersfuture.sg/job/research-associate-national-university-singapore-3221d10d99d0b258bc2f476978aa8e9f,Research Associate,Contract,NATIONAL UNIVERSITY OF SINGAPORE,21 LOWER KENT RIDGE ROAD 119077,Non-executive,Sciences / Laboratory / R&D,40800,84600,Monthly,16 Jan 2019,1,0,0,0,"A Research Associate position is immediately available at the Institute of Data Science, National University of Singapore, in the areas of Data Mining and Machine Learning. The position involves developing and maintaining state-of-the-art data science and machine learning platforms and applications for the institute, and working with data science researchers to conduct research and development on state-of-the-art data science projects in collaboration with industry partners. The job scope includes data preparation and cleaning, designing and implementing of scalable and effective algorithms and methods in data mining, machine learning and artificail intelligence, performing empirical study on real-world data, developing applications and visualization demos etc.   The successful candidate will be a hands-on data engineer with solid coding and application development skills and experience, with a passion and pride to write robust, readable, and reusable code components and applications. He/She should be familiar with at least one ML library/framework, and have up-to-date knowledge in the latest cloud computing and other big data technologies. He/She should also have excellent interpersonal communication skills for working closely with internal and external collaborators to implement data science applications.  Prior R&D exposure would be a plus.","- Master's degree in computer science or related disciplines - More than 2 Years of experience, industry experience would be a plus - Programing language: Python, R, JAVA or C++ - Knowledge and experience in database management and machine learning algorithms - Knowledge and experience in GPU-based computation would be a plus - Experience with various data analysis and visualization tools and big data computing platforms - Experience in developing and deploying machine learning algorithms including deep learning - Experience in Ux design - An independent and self-driven worker and a fast learner - Well-organized and has an eye for details - Excellent written and verbal communication skills"
97,d9dec34e1ae3d1b6df34ccf43fb87c9b,https://www.mycareersfuture.sg/job/big-data-engineer-d9dec34e1ae3d1b6df34ccf43fb87c9b,IT Big Data Engineer,Full Time,Company Undisclosed,,Professional,Information Technology,7000,12000,Monthly,15 Jan 2019,0,0,0,0,"The Big Data Engineer will execute master data management policies developed by the data architect and perform the data quality evaluations. He/She is required to work closely with business representatives to improve the quality of data to the required levels.   Responsibilities  Responsible for the integration of large, structured and unstructured data volumes into the cloud platforms Development of scalable end-to-end data pipelines for batch and stream processing Execution of the datalake integration workflow and activities for populating the data lake and integrating diverse data sources Execution an further development of the physical implementation of the logical data model into a physical implementation in the data lake Implementation of solutions for reference data and master data management within the context of the mobility data business Execution of data quality measurements and implementation of data quality improvement activities to the required levels of data quality Support of build-up and maintenance of a data directory for all data relevant to the mobility data business Representation of the Data Architecture team in selected data architecture, data modelling, and metadata management work teams inside Mobility   "," Diploma or University degree in an appropriate area (e.g. informatics) Relevant work experience Experience with data ingestion tools like Nifi, Streamsets, Node Red Experience with modern big data technologies like Hadoop, MapReduce, Kafka, Hive, Presto, Spark, Storm Experience with cloud solutions like AWS, Azure Experience with programming languages like R, SQL, Scala, Python, Java Experience with NoSql and traditional databases like Mongodb, MSSQL, Hadoop Experience with enterprise application integration and with approaches in one of the leading tool suites (e.g. Kibana, Solr, ElasticSearch, R, Python) Strong technical design and analysis skill Creativity and lateral thinking Ability to deal with ambiguity and work in fast paced environment Deep experience supporting mission critical applications quickly Excellent communication skills, both through written and verbal channels Excellent collaboration skills to work with multiple teams in the organization Ability to understand and adapt to changing business priorities and technology advancements Strong knowledge and technology trends in implementing of Big data ecosystem Strategic thinking and critical problem solving skills "
98,995d39b7e98f157a50df98d041569510,https://www.mycareersfuture.sg/job/data-engineer-leadbook-995d39b7e98f157a50df98d041569510,Data Engineer,Permanent,LEADBOOK PTE. LTD.,"TUNG ANN ASSOCIATION BUILDING, 141 CECIL STREET 069541",Professional,"Information Technology, Sciences / Laboratory / R&D",2000,4000,Monthly,15 Jan 2019,0,0,0,1,"Data Engineer If you are a data science and python enthusiast who likes to build best-in-class python crawlers and data processing pipe, we are interested in you. Leadbook.com is a global leader in customer intelligence. Leadbook helps sales team discover new opportunities, build targeted lists in seconds and gain deep insights about their prospects and customers. Responsibilities:    * Design and develop a highly scalable, data crawlers to extract large volumes of data from www.    * Wrangle the raw data to get cleaned, normalized, and enriched datasets using transformations, normalization and mapping.    * Work in Agile / Scrum environments with remote team    * Develop creative ideas on how to work better and smarter.","Qualifications: 2 years of tangible Python development experience Excellent English communication skills both spoken and written Develop clean, elegant, well-commented, and reusable code with version control (Git) Degree in Computing or equivalent"
99,b6cd750d78ac8eb52fe762130f97b186,https://www.mycareersfuture.sg/job/data-engineer-rakuten-asia-b6cd750d78ac8eb52fe762130f97b186,Data Engineer,Full Time,RAKUTEN ASIA PTE. LTD.,"CAPITAGREEN, 138 MARKET STREET 048946",Manager,Information Technology,4500,8000,Monthly,15 Jan 2019,0,0,0,0," Implement cutting-edge data infrastructure platform which is vendor unlocked and multi-tenant. Implement and manage robust ETL pipeline based on streaming. Implement easy-to-use generalized data accessing layer by leveraging the details of storage engine. Implement distributed machine learning pipeline by coordinating with data science team. Develop data driven culture for integrated partners. Propose new technologies, tools to improve whole process of data system integration. ","Must have  Bachelor degree or higher in Computer Science or related field. Experience in at least one language for web backend application & data processing, such as Java, Python, etc. Experience in NoSQL database, such as Redis, Solr, MongoDB, etc. Experience in Linux system operation, ability to manage system level task such as monitoring and troubleshooting your deployed applications. Good communication skills, ability to work in fast pace R&D. High motivation for learning, skill up, system ownership and contribution to the team.  Must have for Senior position  3+ years of experience in developing large scale data processing platform of various unstructured data. 3+ years of experience in using various big data frameworks and NoSQL databases, such as Hadoop, Kafka, Redis, Solr, etc. Practical knowledge of web system performance tuning including OS, middleware, I/O and application.  Good to have  Experience on cloud computing service, such as AWS. Experience in handling multilingual data. Knowledge in data science domains, such as NLP, Data Mining, and Deep Learning will help your collaboration with the data scientists. "
100,075d88144971d26a7ee20a757f8080f8,https://www.mycareersfuture.sg/job/data-engineer-neuroncredit-075d88144971d26a7ee20a757f8080f8,Data Engineer(Deep Learning),Full Time,NEURONCREDIT PTE. LTD.,,Executive,Information Technology,3500,6500,Monthly,15 Jan 2019,0,0,0,0,"Job Responsibilities  You will work closely with data scientist team to build the training pipeline and improve the computer vision products such as OCR, facial recognition, facial comparison and liveness.","Job Requirements  Bachelor’s Degree in Computer Science/Information Technology/Engineering Good experience with Python is a must Passion in computer vision field with good learning attitude Team player, meticulous, and strong ownership for work "
101,8a1a2ec41c135c42eabd062ad3c83471,https://www.mycareersfuture.sg/job/data-visualisation-engineer-8a1a2ec41c135c42eabd062ad3c83471,IT Data Visualisation Engineer,Full Time,Company Undisclosed,,Professional,Information Technology,7000,12000,Annually,15 Jan 2019,0,0,0,0,"If you have a passion for data analytics, and wants to make an impact by designing and building visually intuitive, information-rich data products, this is your opportunity. In this position, you will be a torchbearer for data visualisation and UI design, and the incredible results it brings in organizations.   Responsibilities  Shape our data visualisation and UI design consulting business by leading business development efforts and consulting projects Gathering and documenting client requirements and translating these into process and UI architecture designs Providing thought leadership and actively participating in the application design, implementation, and roll-out of the solutions Act as a trusted advisor for data visualisation solutions and services for our clients and prospects Design and develop eye-catching data visualisation demos and storylines, adding to and expanding the services Spearhead the evangelisation of and demonstrate the power of data visualisation and UI design by teaching, leading community activities and by building reusable assets "," Relevant data visualisation and UI design experience (i.e. visualising complex data, designing data-rich user interfaces, data-driven design and development of charts, dashboards and infographics etc.), ideally in a consulting function Result oriented self-starter, with an ability to lead and mentor other team members Experience in designing, building, deploying and maintaining scalable, highly available and performance optimized data visualization solutions for enterprise Combine an analytical and creative approach to problem solving and have business strategy skills to ask the right questions and find the right answers to build the solutions necessary Be a clear, confident and persuasive communicator, with excellent presentation skills and with the ability to structure a coherent, logical argument and the confidence to defend assumptions, projections and recommendations Strong client communication and consulting skills. Should be able to lead conversations across all levels of an organization Ability to understand and challenge constraints, and recommend alternative choices Solid understanding of analytical data applications, open source solutions, and technologies, e.g. Elastic Stack, Solr, R, etc. A passion for data visualisation and a keen design sense, with an eye for what makes a visual design aesthetically pleasing and intuitive to use Understanding of Design Thinking "
102,61b2e7a6347ebb9e1f4da0a3ed544639,https://www.mycareersfuture.sg/job/research-assistant-national-university-singapore-61b2e7a6347ebb9e1f4da0a3ed544639,Research Assistant,Contract,NATIONAL UNIVERSITY OF SINGAPORE,21 LOWER KENT RIDGE ROAD 119077,Non-executive,Sciences / Laboratory / R&D,36000,60000,Monthly,15 Jan 2019,1,0,0,0,"We have multiple postdoctoral data scientist/data engineer openings for the Grab-NUS AI Lab anchored at the Institute of Data Science at National University of Singapore. Grab (http://www.grab.com/sg/) is Southeast Asia’s leading on-demand transportation platform. The Grab-NUS AI Lab aims to solve transportation challenges with intelligent insights and innovative services enabled by rigorous research in AI and data science. The lab will focus on five key areas: passengers, drivers, traffic, locations, and big data AI platform. We will develop big data-driven machine-learning algorithms to predict and meet the needs of both the passengers and drivers, as well as to model and understand the city’s traffic and its locations better. The lab will also develop a state-of-the-art real-time visual and analytics AI platform to deploy the algorithms on big data. The successful candidate(s) will conduct advance research on AI and data science in one the five focus areas mentioned above, led by distinguished professors from NUS’ Computing Department together with experienced data scientists from Grab.","PhD/MS/BS in Computer Science or related field, with specialization related to data mining, machine learning, or databases; Publications in top-tier conferences in Data Mining, Machine Learning, Databases and other relevant areas; Prior research experience in transportation data analytics would be a plus; Proficiency in large-scale programming systems for big data and AI; Good oral and written skills in English; Experienced in working in a team, with people of diverse skillsets, including industry end-users; Passionate in working with developers and users to get solutions into use. The appointment will be for two years. Selected candidates will be offered with attractive/competitive salaries and benefits. If interested, please send your resume and a cover letter to seekiong@nus.edu.sg."
103,7616f787bea6fc4c9d046896a7879562,https://www.mycareersfuture.sg/job/required-data-centre-engineer-12-months-contract-path-infotech-7616f787bea6fc4c9d046896a7879562,Required Data Centre Engineer-12 Months Contract,Contract,PATH INFOTECH PTE. LTD.,"E-CENTRE @ REDHILL, 3791 JALAN BUKIT MERAH 159471",Executive,Information Technology,3000,3500,Monthly,14 Jan 2019,1,0,0,0,"5 Years working experience in Operation Centre/Data Centre Performing monitoring and issuing of commands to computers and IT equipment’s Working experience of doing shift work would be highly preferred OCT shall be on 12 hours shifts ,performing day shifts and night shifts Alerts signalling abnormalities from various IT domain (eg application, Infrastructure ,network IT Security) are channelled to IOC. IOC if expected to monitor such alerts round the clock and react accordingly i.e OCT is expected to execute well defined routine operational instructions or procedures which are formulated by handed over from various IT domain.","5 Years working experience in Operation Centre/Data Centre Performing monitoring and issuing of commands to computers and IT equipment’s Working experience of doing shift work would be highly preferred OCT shall be on 12 hours shifts ,performing day shifts and night shifts Alerts signalling abnormalities from various IT domain (eg application, Infrastructure ,network IT Security) are channelled to IOC. IOC if expected to monitor such alerts round the clock and react accordingly i.e OCT is expected to execute well defined routine operational instructions or procedures which are formulated by handed over from various IT domain."
104,de3140d8875464b26482d29c4a96ff78,https://www.mycareersfuture.sg/job/data-center-engineer-techcom-solutions-consultancy-de3140d8875464b26482d29c4a96ff78,Data Center Engineer,"Contract, Full Time",TECHCOM SOLUTIONS & CONSULTANCY PTE. LTD.,"SHENTON HOUSE, 3 SHENTON WAY 068805",Senior Executive,"Banking and Finance, Information Technology",5000,6000,Monthly,14 Jan 2019,1,0,0,0," 4+ years working experience in an Operations Centre (OCT), performing monitoring and issuing of commands to computers and IT equipment. Working experience of doing shift work would be highly preferred. OCT shall be on 12-hour shifts, performing day-shifts and night-shifts alternating with teammates in accordance to a shift pattern, with sufficient breaks. Alerts signalling abnormalities from various IT domains (e.g. applications, infrastructure, network, IT security) are centrally channelled to IOC. IOC is expected to monitor such alerts round-the-clock, and react accordingly, i.e. either escalate within SLA to the correct party who can resolve the issue, or execute well-defined instructions or procedures as prescribed or as instructed by domain experts. OCT is expected to execute well-defined routine operational instructions or procedures which are formulated by and handed over from various IT domains, in accordance to an approved daily schedule (which content might differ on different days and on different shifts). OCT is also expected to provide feedback on documents and update when assigned (periodically only), in order to keep them up-to-date. Able to read general IT instructions / procedures (in English) and execute accurately. [This part will be tested during interview session.] "," 4+ years working experience in an Operations Centre (OCT), performing monitoring and issuing of commands to computers and IT equipment. Working experience of doing shift work would be highly preferred. OCT shall be on 12-hour shifts, performing day-shifts and night-shifts alternating with teammates in accordance to a shift pattern, with sufficient breaks. Alerts signalling abnormalities from various IT domains (e.g. applications, infrastructure, network, IT security) are centrally channelled to IOC. IOC is expected to monitor such alerts round-the-clock, and react accordingly, i.e. either escalate within SLA to the correct party who can resolve the issue, or execute well-defined instructions or procedures as prescribed or as instructed by domain experts. OCT is expected to execute well-defined routine operational instructions or procedures which are formulated by and handed over from various IT domains, in accordance to an approved daily schedule (which content might differ on different days and on different shifts). OCT is also expected to provide feedback on documents and update when assigned (periodically only), in order to keep them up-to-date. Able to read general IT instructions / procedures (in English) and execute accurately. [This part will be tested during interview session "
105,ec80cf4678d1bf50b66a251f06283161,https://www.mycareersfuture.sg/job/data-engineer-abakus-ec80cf4678d1bf50b66a251f06283161,Data Engineer,Permanent,ABAKUS (ASIA PACIFIC) PTE. LTD.,,Executive,Information Technology,5500,7000,Monthly,13 Jan 2019,0,0,0,1,"To reinvent an industry, you need to build an all-star team. Join Wecash if you want to realise a world of better living through responsible credit, by developing and promoting products that provide businesses with comprehensive and accurate evaluation of consumer credit worthiness as well as underwrite loans between funding sources and consumers. Wecash leverage upon the power of mobile technology, big data and machine learning to prevent fraud and determine consumer credit worthiness. Wecash started in 2014 and has since raised more than US$200 million in financing, acquired over 100 million users and processed more than US$5 billion in loans in 2017 alone. Wecash is creating a platform for financial products for the financially underserved utilizing artificial intelligence, big data and mobile technology. At Wecash, we move really quickly, get stuff done, and are constantly iterating. We are looking for a data engineer to support our expansion in Southeast Asia who will collaborate with the Data Science team to streamline our data products for analytics, operation and research. The candidate should be proactive and willing to keep-up with cutting-edge technologies. Responsibilities:  Develop, deploy and maintain machine learning models Design simulation and testing frameworks for model validation Identify, collect and organize new data sources Research on new technologies for efficient data management   "," Degree in Computer Science/Mathematics/Statistics/Physics, or related fields Proficiency in Python, SQL and Linux shell with minimum 1 year of hands-on experience Experience in statistical modelling Knowledge in Web Frameworks and RESTful APIs Knowledge in Version Control and Continuous Integration tools Willing to learn, innovative, good communication skills and team player  Bonus Skills  Working experience in NoSQL Working experience in Machine learning Ability to communicate in Mandarin or Bahasa Indonesia or other Asian language to communicate with counterparts in China, Indonesia and other APAC offices. "
106,25e186a613429a46f1cf306ce5478751,https://www.mycareersfuture.sg/job/data-engineer-observational-pragmatic-research-institute-25e186a613429a46f1cf306ce5478751,Data Engineer,"Permanent, Full Time",OBSERVATIONAL AND PRAGMATIC RESEARCH INSTITUTE PTE. LTD.,"SOUTHBANK, 883 NORTH BRIDGE ROAD 198785",Junior Executive,"Engineering, Information Technology",3000,6000,Monthly,11 Jan 2019,0,0,0,1,"The Company OPRI is an academic research institution striving to improve the lives of patients through global research. OPRI has been leading the paradigm shift in real world evidence for the past 12 years, by delivering pragmatic clinical trials, disease registries and database research. The Role We are looking for a Data Engineer to work alongside our research, statistical and database teams in the UK, Singapore and Australia (Brisbane). In this position you will gain invaluable experience within an internationally recognised research organisation involved in analysis and dissemination of data from large-scale observational studies and pragmatic randomised controlled trials. The successful candidate will have high attention to detail, strong time management skills, and most importantly experience in the management and engineering of relational databases. Your responsibilities  Design, construct, install, test and maintain data collection and management systems:     Integrate data management technologies and software engineering tools for custom data collection applications Programming knowledge: Employ a variety of languages and tools (e.g. scripting languages) to combine systems together Ensure seamless integration of data across multiple databases       SQL, queries   Building APIs for data consumption Integrating external or new datasets into existing data pipelines Continuously monitoring and testing the system to ensure optimized performance   Build and maintain data collection platforms for specific organisational projects     Set up automated integration processes for Patient Reported Outcomes into various data collection platforms   EMR/EDC integration with Registry Database     Data collected via Registry EDCs to be uploaded into EMRs Data collected via site specific EMRs/EDCs to be uploaded into Registry EDCs    The role is for a permanent full-time position. Salary is dependent on qualifications and experience. Immediate start is available.","Qualifications  Bachelor degree in Computer Science, Engineering, Maths or equivalent qualification  Required Experience  Strong working knowledge of SQL (Essential) Experience working with large databases  Preferred Experience  Experience of developing and maintaining data dictionaries for databases Knowledge of statistical analysis tools (e.g. R, STATA, SPSS, SAS) Interest and knowledge of epidemiology, public health and clinical research "
107,e3e85a4ec5ef7433d78457b22da2085e,https://www.mycareersfuture.sg/job/data-engineer-ninja-logistics-e3e85a4ec5ef7433d78457b22da2085e,Data Engineer,"Permanent, Full Time",NINJA LOGISTICS PTE. LTD.,"KEWALRAM HOUSE, 30 JALAN KILANG BARAT 159363",Middle Management,"Engineering, Information Technology",4500,6000,Monthly,11 Jan 2019,0,0,0,1,"  Design, develop and maintain Ninja Van’s infrastructure for streaming, processing and storage of data. Build tools for effective maintenance and monitoring of the data infrastructure.   Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives.   Work closely with stakeholders to develop scalable and performant solutions for their data requirements, including extraction, transformation and loading of data from a range of data sources.   Develop the team’s data capabilities - share knowledge, enforce best practices and encourage data-driven decisions.   Develop Ninja Van’s data retention policies, backup strategies and ensure that the firm’s data is stored redundantly and securely.   Tech Stack   Data storage: Percona XtraDB Cluster, Elasticsearch, Apache Cassandra   In-Memory data grid: Hazelcast   Real-time data pipeline: Apache Kafka   Backend webservice stack: Play (Java 8), GoLang, Node.js   Web frontend: AngularJS, React   Mobile: Android SDK, React Native   Containerization: Docker on Kubernetes  ","  Solid Computer Science fundamentals, excellent problem-solving skills and a strong understanding of distributed computing principles.   At least 3 years of experience in a similar role, with a proven track record of building scalable and performant data infrastructure.   Expert SQL knowledge and deep experience working with relational and NoSQL databases (e.g. HBase, Cassandra).   Advanced knowledge of Apache Kafka and demonstrated proficiency in Hadoop v2, HDFS, MapReduce.   Experience with stream-processing systems (e.g. Storm, Spark Streaming), big data querying tools (e.g. Pig, Hive) and data serialization frameworks (e.g. Protobuf, Thrift, Avro).   Bachelor’s or Master’s degree in Computer Science or related field from a top university.  "
108,189dd8637b4193915e4a37247baa461c,https://www.mycareersfuture.sg/job/data-engineer-grabtaxi-holdings-189dd8637b4193915e4a37247baa461c,Data Engineer,Full Time,GRABTAXI HOLDINGS PTE. LTD.,"OUE DOWNTOWN, 6 SHENTON WAY 068809",Professional,Engineering,6000,10000,Monthly,11 Jan 2019,0,0,0,0," Build, deploy and manage big data solutions that can adequately handle the needs of a rapidly growing data driven company  Spearhead the development of systems, architectures, and platforms that can scale to the 3 Vs of Big data (Volume, Velocity, Variety)   Streamline data access and security to enable data scientists and analysts to easily access to data whenever they need to   Build out scalable and reliable ETL pipelines and processes to ingest data from a large number and variety of data sources   Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making     Lead the movement cleaning and normalizing subsets of data of interest as preparatory step before deeper analysis by the data scientists   Run Modern high performance analytical databases and computation engines like RedShift, BigQuery, Greenplum,Presto and others  ","  A degree or higher in Computer Science, Electronics or Electrical Engineering, Software Engineering, Information Technology or other related technical disciplines.   Experience in handling large data sets (multiple TBs) and working with structured, unstructured and geographical datasets   Designed high performance scalable infrastructure stacks for Big Data Analytics   Deep understanding of databases and best engineering practices - include handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline   Real passion for data, new data technologies, and discovering new and interesting solutions to the company’s data needs   Excellent communication skills to communicate with the product development engineers to coordinate development of data pipelines, and or any new products features that can be built on top of the results of data analysis  "
109,a864322e752c5980eb08977bbe744d8c,https://www.mycareersfuture.sg/job/data-centre-engineer-operator-itcan-a864322e752c5980eb08977bbe744d8c,Data Centre Engineer  /  Operator,Full Time,ITCAN PTE. LIMITED,"PRUDENTIAL TOWER, 30 CECIL STREET 049712","Professional, Non-executive",Information Technology,2500,3500,Monthly,10 Jan 2019,0,0,0,0," Ensure tasks in Ops daily checklist are being carried out in a timely manner Coordinate Data Centre incidents, maintenance and shutdowns Coordinate incident management Accountable for any emergency and logon id accounts, issue or use it according to instruction and procedure given and record its usage during the shift Control of keys kept with operations and record its usage Accountable for all physical security access cards available in the Data Centre   Responsible for equipment movement in Data Centre Coordinate preventive maintenance for hardware equipment with vendors, where applicable Ensure physical environment in the Data Centre is in normal function (e.g.. UPS, temperature control, humidity, etc.) Account for the inventory of hardware equipment and manage external vendors providing the support Manage the roster in the shift Ensure incidents are responded and attended to, else redirect for 2nd/3rd level resolution base on criticality, impact and SLA. Monitor and ensure timely completion of scheduled batch jobs at end of day Manage and mitigate escalations of high impact system failures and assist in tracking the incidents "," Diploma with 2 years into Data Centre environment Troubleshoot Data Centre systems/networks to diagnose a problem and administer Perform backup on all systems to ensure recovery of important data in event of system failure. Install Hardware, server, Switches and Appliance. "
110,843285eddedf6ceef0541d567426f31a,https://www.mycareersfuture.sg/job/application-engineer-smart-systems-ai-data-technology-infineon-technologies-asia-pacific-843285eddedf6ceef0541d567426f31a,Application Engineer - Smart Systems with AI & Data Technology,Permanent,INFINEON TECHNOLOGIES ASIA PACIFIC PTE LTD,"INFINEON, 8 KALLANG SECTOR 349282","Fresh/entry level, Professional, Junior Executive",Engineering,3000,5500,Monthly,10 Jan 2019,0,0,0,1," Be a member of the Systems team that delivers embedded reference solutions / kits using Infineon semiconductors, microcontrollers Have particular focus to apply artificial intelligence and data technologies and to implement smart connected systems Be responsible for specification, set up and implementation of solution comprising of hardware / circuits and application software, towards target application Deliver on required components to complete a reference solution / demo / kit – including documentation Maintain close cooperation with Marketing, Application teams and Field Application Engineers "," University degree in Computer Science or Electrical Engineering No prior employment required if you are an active technical hobbyist Coding experience in Python  Experience with embedded system development using C-language Understanding of electronics / electrical circuits, can debug the system Experience in embedded Linux is preferred Microcontroller and standard peripherals know-how is preferred A team self-motivator, demonstrated ability to independently explore and acquire new technologies to enable his project Able to travel occasionally when required "
111,1f257f3fe7b5eebd4832a6d4f3f9bfe6,https://www.mycareersfuture.sg/job/avp-data-engineer-ibg-digital-institutional-banking-group-dbs-bank-1f257f3fe7b5eebd4832a6d4f3f9bfe6,"AVP, Data Engineer, IBG Digital, Institutional Banking Group (1800044U)","Permanent, Full Time",DBS BANK LTD.,,"Manager, Senior Executive",Banking and Finance,6500,11700,Monthly,09 Jan 2019,0,0,0,1,"Job Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management "," Master’s Degree in software Engineering, Computer Science or related fields with minimum 3 years data engineering work experience in big data analytics environment Strong in data engineering skills with big data stack (Hadoop, Spark, Kafka, etc) Strong in transactional SQL, Enterprise Data Warehouse Experience with Graph Database, NoSQL databases Experience with Feature Engineering Experience with Master Data Management Experience with scripting languages: UNIX/Linux Shell, SQL, Python (Pandas, PySpark etc), Scala, R, etc "
112,89827330ea0c49864b2a6ea27caa1932,https://www.mycareersfuture.sg/job/senior-associate-data-engineer-ibg-digital-institutional-banking-group-dbs-bank-89827330ea0c49864b2a6ea27caa1932,"Senior Associate, Data Engineer, IBG Digital, Institutional Banking Group (1800044U)","Permanent, Full Time",DBS BANK LTD.,,"Manager, Senior Executive",Banking and Finance,5000,9000,Monthly,09 Jan 2019,0,0,0,1,"Job Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management "," Master’s Degree in software Engineering, Computer Science or related fields with minimum 3 years data engineering work experience in big data analytics environment Strong in data engineering skills with big data stack (Hadoop, Spark, Kafka, etc) Strong in transactional SQL, Enterprise Data Warehouse Experience with Graph Database, NoSQL databases Experience with Feature Engineering Experience with Master Data Management Experience with scripting languages: UNIX/Linux Shell, SQL, Python (Pandas, PySpark etc), Scala, R, etc "
113,cb7271da6bec8cfefa49c74f9184a096,https://www.mycareersfuture.sg/job/data-engineer-machine-learning-engineer-oneconnect-financial-technology-cb7271da6bec8cfefa49c74f9184a096,Data Engineer / Machine Learning Engineer,Permanent,ONECONNECT FINANCIAL TECHNOLOGY (SINGAPORE) CO. PTE. LTD.,"ONE RAFFLES PLACE, 1 RAFFLES PLACE 048616",Professional,Engineering,3000,4000,Monthly,08 Jan 2019,0,0,0,1,"As a data/Machine learning engineer, your primary goal is to work with data scientist and software developers to implement and deploy machine learning algorithms driven data solutions in commercial environment. You’ll work with data scientist to build specialized tools to facilitate data preprocessing, data quality check, data cleansing, ETL and data integration. You’ll work with backend and frontend app-dev team to implement high performance machine learning algorithms on commercial enterprise software platform. Key requirement:  Advanced degree in Computer Science, Electronic Engineering, Statistics, Applied Mathematics, or related technical fields. Minimum 2 years relevant experience. Proficiency in a high performance programming language such as Java/C++. Good working knowledge in a scripting language such as Python/R. Experience implementing algorithms in big data/cloud computing environment. Experience in solving real-world data challenges and dealing with anomalies. Good knowledge working with relational database and NoSQL database Good communication skills ","Preferred requirement:  Experience building labeled datasets from scratch, or developing tools protocol formalizing unstructured unlabeled data. Experience handling high-volume real-time data stream Experience building complete data pipeline, ingesting data from multiple source systems (potentially asynchronous and different rate) Experience building large scale parallel/distributed data processing, machine learning solutions Experience building robust and high-throughput API for analytics-as-a-service solutions Experience working with enterprise app engine technology such as cloud computing, container technology for high availability and robust deployment. "
114,9a2882ca74c6e52d5a8f33fbc7a5a572,https://www.mycareersfuture.sg/job/data-engineer-moka-technology-solutions-9a2882ca74c6e52d5a8f33fbc7a5a572,Data Engineer,"Permanent, Full Time",MOKA TECHNOLOGY SOLUTIONS PTE. LTD.,,Professional,Engineering,5000,7000,Monthly,08 Jan 2019,0,0,0,1,"Do you have a passion for data? Are you looking to push the frontiers of innovation and build the Next Big Data Product? We are looking for excellent Data Engineers who are keen to help us manage the end-to-end data pipeline and drive big data solutions.  You will:  Design, implement and manage end to end data pipelines (ETL, data streaming and warehousing) so as to make data easily accessible for analysis. Integrate with third party APIs for accessing external data. Create and maintain data warehouses for reporting or analysis. Consult and partner with engineering and product teams to execute data-related product initiatives. Ability to quickly resolve performance and systems incidents. Evaluate the latest monitoring and automation tools. ","You have:  BS (MS preferred) in Computer Science or Computer Engineering. Excellent software engineering skills and proven track record (4+ years experience) in building automated, scalable and robust data processing systems. Proficiency in SQL, bash scripts and Python (or similar languages). Intermediate understanding of database technologies. Experience with data warehouse systems (e.g. Redshift) and batch/semi-online building blocks (e.g. MapReduce, Spark etc). Demonstrated expertise in working with large scale quantitative data. Excellent attention to detail and team player. "
115,a16594f4efee319e982c895bbf67e8ed,https://www.mycareersfuture.sg/job/data-engineer-prudential-assurance-company-singapore-a16594f4efee319e982c895bbf67e8ed,Data Engineer,Permanent,PRUDENTIAL ASSURANCE COMPANY SINGAPORE (PTE) LIMITED,"PRUDENTIAL TOWER, 30 CECIL STREET 049712",Senior Executive,"Engineering, Information Technology, Insurance",4000,6500,Monthly,08 Jan 2019,0,0,0,1,"Job Description Summary In this role, you will design, develop and provide support for various data systems (based on both traditional RDBMS and big data platform) in Prudential Singapore. As part of this dynamic role, you will report to Lead System Analyst, and work closely with business units and other IT teams to deliver leading edge technology to enable digital capabilities of Prudential Singapore.  Analyse business needs in order to design, develop and deliver data solutions to meet business objectives Provide application maintenance and support for various data systems in accordance to Service Level Agreement Deliver data solutions in accordance to relevant IT policies and procedures Provide and share knowledge to other team members ","Competencies & Personal Traits    Independent and works well across different functions   Excellent problem analysis skill. Innovative and creative in developing solutions   Strong sense of drive and commitment to deliver on responsibilities   Strong verbal and written communication skills   Works well in a dynamic environment   Ability and willingness to be hands-on   Experience in 2 or more of the following technology: 	 RDBMS: Oracle, Postgres, MSSQL, Netezza ETL and Data Integration Tools:  IBM Datastage, Pentaho, Talend, Attunity BI Tools: PowerBI, Qlik Big Data: Hadoop (Hortonworks), Hive QL, Spark, Sqoop, etc Programming and Scripting: Linux/Unix Shell Scripting, Java, Scala, Hive QL, PL/SQL, NoSQL      Working Experience:    2-8 years in designing and developing and support applications. Fresh graduates are welcome to apply   Experience in Agile software development    Education    Bachelor in Computer Science, Computer Engineering or equivalent   "
116,6ce1bdabdd4547c748dbc3a13f1ace8a,https://www.mycareersfuture.sg/job/data-center-operations-engineer-mcits-technologies-6ce1bdabdd4547c748dbc3a13f1ace8a,Data Center Operations Engineer,"Contract, Full Time",MCITS TECHNOLOGIES PTE. LTD.,"TONG BUILDING, 302 ORCHARD ROAD 238862",Executive,Information Technology,2500,3600,Monthly,08 Jan 2019,1,0,0,0,"  Minimum GCE ‘O’ Level holder or diploma, preferably in Information Technology discipline, or equivalent required  Applicant with minimum of 3 to 5 years’ Data Center experience and supporting multi-vendor environment preferred  Experience in BMC Control-M Batch Scheduler and Veritas Netbackup preferred  Must be detail oriented, highly organized and able to multi task in an efficient manner  Applicant with knowledge of Sun Solaris, HP UX, MS Windows and Microsoft Office mandatory  Familiar with hardware/software components and terminology preferred  Experience in analyzing hardware and software problems and making quick diagnosis preferred  Must be willing to work in a rotating shift and additional hours to support the team as required, including major public holidays  Strong ethics, integrity and genuine concern for the needs of others. Must be a team player  Applicant with customer service background desired but not a must  Ability to work in a team environment, and also able to work under pressure with minimum supervision  Good initiatives, proactive with good command of spoken and written English "," Batch Job Management   Ensure Daily Batch Jobs for Billing and Revenue Collection are executed correctly and on time using a Enterprise Batch Scheduling tool.  Monitor the progress of jobs execution and escalation to Application Support when necessary.  Provide 1st Level Incident Management/Troubleshooting and act as point of contact with Banks and Merchants to resolve issues.  Ensure batch processing is on schedule and monitor all system and batch jobs status.  Respond to all user enquiries regarding system functionalities or billing and payment processing.  Participate in UAT to ensure that new job function(s) are adhered to defined standards and specification.  Tape Backup and Management   Ensure daily tape backup are executed as scheduled and completed successfully.  Provide 1st Level Incident management/troubleshooting and escalate to engineering support when necessary.  Provide backup jobs status/progress monitoring.  Facility Management   Ensure Data Centre is operationally 24 x 7.  Responsible for physical security of the Data Center as defined in the Corporate Security Policy and standards.  Ensure Data Centre air conditioning units, lights and temperature are functional and within normal operational standards.  Work as part of a team providing coverage on a 7 day a week, 24 hour per day (24/7) basis including public holidays, on a 12 hour rotating shift basis.  Remote Hand Service   Provide vendor escorting service when in StarHub secured facilities.  Provide basic cabling support like loose cable verification, connecting pre-laid cable – patching, etc.  Provide visual checks and verifications of equipment (power cycle, equipment indicators, inspection of equipment, hardware reset.)  Provide basic checks on environmental conditions in the Data Centre.  Provide basic data media loading/unloading.  Provide hardware replacement of systems (parts replacement – interface card & installation, slotting/removal of blades/line cards, movement of equipments).  Assist in the loading/unloading of tapes to StarHub Tape Library using an enterprise backup application.  Assist in the labelling of Tape media.  Keep inventories of the movement of tape using TMS (Tape Management System). "
117,3a3c70eb175dd9ae8a2f7e8e44fa9c60,https://www.mycareersfuture.sg/job/research-engineer-astar-research-entities-3a3c70eb175dd9ae8a2f7e8e44fa9c60,Research Engineer (Big Urban Data),"Contract, Full Time",A*STAR RESEARCH ENTITIES,,Non-executive,Sciences / Laboratory / R&D,2500,5000,Monthly,08 Jan 2019,1,0,0,0,He/She will mainly work on data preprocessing and preliminary analysis for dengue risk modeling research in this project. He/She will also be responsible for database construction and maintenance in this project and assist research scientist to fulfil the research tasks and deliverables.," Master/Bachelor Degree in mathematics, engineering and electronics With good programming experiences and capabilities in python, c sharp, R and Java Experience in data analysis, parallel computing, optimization, agent-based simulation, and/or visualization is an added advantage "
118,b7c0e2d50e0b78bef8e80534fc7c81e0,https://www.mycareersfuture.sg/job/avp-senior-associate-data-analyst-analytic-center-excellence-transformation-grp-dbs-bank-b7c0e2d50e0b78bef8e80534fc7c81e0,"AVP  /  Senior Associate, Data Analyst, Analytic Center of Excellence, Transformation Grp (180003G5)","Permanent, Full Time",DBS BANK LTD.,,"Manager, Senior Executive",Banking and Finance,5500,11000,Monthly,08 Jan 2019,0,0,0,1,"Job Purpose The data analyst will provide the big data analytic support to the Analytic Center of Excellence. He will partner with business and project leader to discover, analyse and process the data to develop analytic and data science solutions. This position allows those with strong data analytic skill and theoretical understanding of advanced analytic algorithms but lack of hands on experience in advanced analytics to learn and prepare for the role of data scientist - advanced analytics in the future. Responsibilities   Identify, profile, analyze and present the data discovery output for analytic projects Develop data ingestion pipeline and create the analytic data assets for analytic projects Work with data engineer to enhance the analytic data infrastructure and develop enterprise analytic data mart Perform data wrangling and feature engineering for machine learning Create helper functions to automate frequently encountered wrangling and feature engineering tasks "," Bachelors/Masters in Computer Science, Statistics, Mathematics and other highly quantitative fields such as bio-informatics Minimum 3 years of industry experience in data analytics working in a big data environment, preferably in financial services industry Highly proficient with data wrangling, analytic, transformation and feature engineering using programming tools such as Spark, Python or R. Excellent knowledge of SQL. Excellent visualization and communication skills. "
119,1b9774ae6562ff847639b9148fc59f6f,https://www.mycareersfuture.sg/job/bigdata-engineer-helius-technologies-1b9774ae6562ff847639b9148fc59f6f,Bigdata Engineer,Contract,HELIUS TECHNOLOGIES PTE. LTD.,"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",Professional,Information Technology,6500,8500,Monthly,08 Jan 2019,1,0,0,0,"We are looking for a Senior Big data engineer with python background to help us build and evolve Data Management Platform. Your primary focus will be the development of all server-side backend logic, ensuring high performance and responsiveness to requests from the front-end / API requests.","Big Data:  2+ years of working experience in enterprise level big data projects. Should have strong understanding of Distributed computing environment In-depth knowledge of  Hadoop ecosystem tools (HDFS, YARN, MapReduce, Hive, Zookeeper, Amabri, etc.) Scripting Data Pipelines (Apache Hive / PIG) Good understanding of  Storm and MongoDB Architecture Required  Python:  5+ years of experience working  in the enterprise setting developing python applications in multiple projects. Experience writing python applications that interact with  ORM (Object Relational Mapper) libraries Able to integrate multiple data sources and databases. Strong unit test and debugging skills Strong knowledgeable in XML and JSON parsing in python. Developing ETL processing in python as needed.  Other desired skills:  Added advantage if the candidate has  java  experience/background. Basic  experience with SQL, data analysis and linux shell scripting. Knowledge in Kafka, message queues. Worked with rest APIs. Proficient understanding of code versioning, unit testing tools (e.g. Git) An excellent team player and communicator who can work effectively with cross functional teams. Flexible to work in PST or SGT on weekdays and be On-call during weekends in shift model as situation demands. Willingness to learn new tools as needed. "
120,2215e8a8684aaa50c17489883ae473b0,https://www.mycareersfuture.sg/job/data-engineer-fixed-mobile-2215e8a8684aaa50c17489883ae473b0,Data Engineer,Permanent,FIXED & MOBILE PTE. LTD.,"ANSON HOUSE, 72 ANSON ROAD 079911",Non-executive,Information Technology,4000,7000,,07 Jan 2019,0,0,0,1,"TransferTo operates a leading global digital value services network offering safe, reliable and more accessible mobile value services for emerging markets. Our B2B cross-border network interconnects and provides mobile operators, money transfer operators, retailers, distributors, NGOs and corporates with unparalleled infrastructure and reach to offer solutions that better connect loved ones across borders. A career with TransferTo provides invaluable experience in an exciting and rapidly expanding market and an opportunity to be part of a truly global company with 7 offices worldwide and a workforce that includes over 50 different nationalities. The Data Science & Data Engineering department is focusing on the creation of new data sciences capabilities for the business by envisioning and executing strategies that will improve performance by enabling informed decision making. We are seeking an energetic and passionate data engineer to help build the robust foundations that will support current and future data-intensive computations across the company. As a Data Engineer, you will be working closely with the Infrastructure, Software development, Business Intelligence and Data Science teams. You will have the opportunity to shape our data stack, ensuring that our ever-flowing data is adequately collected, organized and made accessible for advanced analytics and beyond. Key Role Responsibilities  Build a reliable, scalable and efficient cloud-based data processing platform and applications Play a central role in delivering our next generation real-time, big data platform Conceive, develop, monitor and optimize reliable data pipelines that convert data into insights Be involved in whole data platform development process including infra, data ETL and service implementation  About Us TransferTo operates a leading global digital value services network offering safe, reliable and more accessible mobile value services for emerging markets. Our B2B cross-border network interconnects and provides mobile operators, money transfer operators, retailers, distributors, NGOs and corporates with unparalleled infrastructure and reach to offer solutions that better connect loved ones across borders. For more information, visitwww.transfer-to.com/digitalvalue","Essential Experience  More than 3 years of experience in data engineering Advanced knowledge of real-time data streams, ETL processes and how to clean, structure and manage sensitive data effectively Strong development skills in some of the following languages: R, Python, Hive, Pig, Mahout, Java, ... Working knowledge of big data concepts like Hadoop, Spark, MapReduce, and HDFS Deep understanding of both SQL and NoSQL data stores Familiar with data tools and services in Azure, AWS, and/or GCP eco-system is preferred Familiar with data management and visualization tools such as Tableau Knowledge of Amazon AWS services and their applications (RDS, Redshift, S3, EC2, ...) Knowledge in RESTful API development Knowledge in CI tools like Jenkins Knowledge in QA processes and automation Coursework in machine learning, data science, data mining, big data, and/or statistical inference is a plus Comfortable with GIT version control Excellent written and verbal communication skills in English A DevOps attitude - you build it, run it & maintain it The ability to execute independently Enjoy learning and adopting new technologies to push the team forward Thrilled to find creative solutions for the challenges you face Finally, wanting to have responsibilities that make a significant contribution and impact on the company "
121,b1e875e5bdf56b69b285c64dccfc666c,https://www.mycareersfuture.sg/job/data-engineer-hooq-digital-b1e875e5bdf56b69b285c64dccfc666c,Data Engineer,Permanent,HOOQ DIGITAL PTE. LTD.,"COMCENTRE, 31 EXETER ROAD 239732",Manager,"Engineering, Information Technology",,,Monthly,07 Jan 2019,0,0,0,1,"We are looking for a Data Engineer to join our rapidly expanding Data & Analytics team. You will help shape how we build and grow our service in the region. We look for self-starters who demand the best. Key Responsibilities:   Develop ETL/ELT jobs to integrate new data into the data warehouse or build new reporting schemas   Develop data pipelines, both bath and realtime from various platforms into the data lake   Manage various data platforms and seek out new technologies to improve efficiency   Develop advanced analytical models that help the business identify trends within customer base and behavior   Work closely with the business to understand data needs and create data sets to enable reporting and dashboards to monitor business performance   Ensure the data warehouse load jobs run as per schedule and data availability to the business is uninterrupted   Continuously improve the information management platforms of the company to leverage benefits ","Desired Skills and Experience  Minimum 3 years of solid development experience within a data warehouse/information management team with strong understanding of programming languages like Java, Python, JavaScript and SQL.   Hands on experience in full-stack development, design and architecture. Experience in creating a REST API that can handle a production load (code + deploy).   Familiarity with AWS (DynamoDB, Redshift, S3, EC2, RDS, Lambda) (Will be an advantage but not mandatory)   Minimum 3 years development experience with ETL/ELT tools (preferably Pentaho DI, Informatica, Datastage or Talend)   Proven experience with Data Warehousing and Big Data technologies   Working knowledge of big Data Technologies like Hadoop, Hive, Spark and streaming/messaging services like Kafka,Spark streaming.   Solid understanding of some BI tools such as Cognos, QlikView or Tableau.   Comfortable working in dynamic fast paced environment with competing priorities. Self-starter and willing and able to learn on your own   Work well within a team environment and willing to accommodate task and duties that maybe outside of your JD for limited time periods "
122,b95b9c1978e790c94a48ba244a7a8edd,https://www.mycareersfuture.sg/job/data-technical-support-engineer-talent-trader-group-b95b9c1978e790c94a48ba244a7a8edd,Data Technical Support Engineer,"Permanent, Full Time",TALENT TRADER GROUP PTE. LTD.,"GATEWAY EAST, 152 BEACH ROAD 189721",Senior Executive,Information Technology,2500,4000,Monthly,07 Jan 2019,0,0,0,1," 1st level support on transmission, VoIP services and switching equipment  Manage ticket queue and responsibility for the issues Provide technical support through emails, tickets and phone Manage the network and systems for internal issues Responsible for support and maintenance for tracking, documentation and verification Arrange the roster shift for the team members "," Hands-on experience in network concepts Comfortable to work in Data Centre environment Independent, self-motivated and dynamic personality Experience in Data Centre NOC    Interested candidates who wish to apply for the advertised position, please email us an updated copy of your resume to; Email Address: it@talenttradergroup.com   EA License No.: 13C6305 Registration ID: R1333012   For candidate who applied for the advertised position is deemed to have consented to us that we may collect, use or disclosed your personal information for purpose in connection with the services provided by us.  "
123,7419e3f2257cd519bb62f11f7fdfc090,https://www.mycareersfuture.sg/job/data-centre-technical-engineer-7419e3f2257cd519bb62f11f7fdfc090,Data Centre Technical Engineer (Ref 22800),"Contract, Full Time",Company Undisclosed,,Executive,Information Technology,3000,6000,Monthly,07 Jan 2019,1,0,0,0,"- Provide infrastructure and security monitoring/support for a 24x7 data centre.  - Carry out service requests which include code deployment, restart servers - VM - backup - storage, manage batch job processing, perform security scanning, escort duty etc.   ","  - At least 3-4 years of relevant working experieces - Experiences in IBM System Storage, Microsoft Windows Server Administration, NetBackup Backup Operations         Licence No: 12C6060"
124,2971a7516b523a443afe3df36cb834a9,https://www.mycareersfuture.sg/job/cyber-security-big-data-engineer-2971a7516b523a443afe3df36cb834a9,Cyber Security Big Data Engineer,Full Time,Company Undisclosed,,Professional,Information Technology,7000,12000,Monthly,03 Jan 2019,0,0,0,0,"Working in Cybersecurity takes pure passion for technology, speed, a constant desire to learn, and above all, vigilance in keeping every last asset safe and sound. You’ll be on the front lines of innovation, working with a highly-motivated team laser-focused on analyzing, designing, developing and delivering solutions built to stop adversaries and strengthen our operations. Your research and work will ensure stability, capacity and resiliency of our products in emerging industry trends. Working in tandem with your internal team, as well as technologists and innovators across our global network, your ability to identify threats, provide intelligent analysis and positive actions will stop adversaries and strengthen our products.   Responsibilities  Focus on the development of tools and technologies that are at the core of the company’s capabilities to manage, monitor and hunt for cyber security incidents Architecture and development of large scale solution (big data) to be used in a very large production environment System, network and application troubleshooting Provide engineering support for cyber security products developed "," Knowledge of Cybersecurity organization practices, operations, risk management processes, principles, architectural requirements, engineering and threats and vulnerabilities, including incident response methodologies Ability to collaborate with high-performing teams and individuals throughout taghe firm to accomplish common goals Proficiency in the use of skills tools, staying current with skills, participating in multiple forums Experience with Agile and can work with at least one of the common frameworks is highly desired Ability to analyze vulnerabilities, threats, designs, procedures and architectural design, producing reports and sharing intelligence Strong research, analytical and problem solving skills Independent problem-solving, highly motivated and self-directing Ability to write and debug administrative and reporting tools in some programming languages (Shell/Perl or Python, Scala/Java/R, C/C++, HTML5, or other experiences acceptable) Comfortable with most aspect of operating system administration such as tweaking, hardening and configuring services A solid understanding of Unix-based operating systems, including paging/swapping, IPC, drivers and filesystem (inode, partitions, etc.) Experience with host and network security (identity/password management, ACLs, file permissions and integrity) Strong interpersonal and communication skills; capable of writing documentation, training users in complex topics, making presentations to junior and very senior audience Ability to work under pressure in a fast-paced environment while remaining productive and professional; exercise patience and ability to multi task    Bonus Points  Experience with hadoop ecosystem: Hadoop, Spark, Map/Reduce, Hive/Pig, Impala/Drill, etc. Experience with Data Science: MLlib, Scikit, h2o, TensorFlow, Pytorch, Caffe, Singa, etc. Experience with NoSQL stacks: Elasticsearch, MongoDB, etc. Experience with SIEM products: Qradar, Arcsight, Splunk, etc. Experience with messaging and data transport tools: Kafka, NiFi, LogStash, Syslog-ng, rsyslog, etc. Experience with Link Analysis tools and GraphDBs Experience with data visualization tools: Hue, Kibana, Qlikview, Tableau, etc. Knowledge in RIA: HTML5, node.js, bootstrap, angular, extJS, etc. "
125,893461e81cb7a2343b546240e2afb8ee,https://www.mycareersfuture.sg/job/facilities-engineer-supreme-hr-advisory-893461e81cb7a2343b546240e2afb8ee,Facilities Engineer (Data Centre / M&E / Facility Management / Bishan),Permanent,THE SUPREME HR ADVISORY PTE. LTD.,"AZ @ PAYA LEBAR, 140 PAYA LEBAR ROAD 409015",Executive,Engineering,2500,4000,Monthly,03 Jan 2019,0,0,0,1,"• Attractive salary packages • Company Bonuses, Benefits & Privileges • Career Progression Opportunities   Interested applicants can send your resume to supreme.cathrynteng@gmail.com and allow our Consultants to match you with our Clients. No Charges will be incurred by Candidates for any service rendered.   Role:  Daily health check non-critical/critical facilities & records. Attendance/initial troubleshooting to fault/breakdown/complaint & records. Attend user request for handyman works, eg. Cabinet door faulty, minor shifting etc and report to client CAS for follow up action. Generating the 1st incident report & escalation. Coordination/supervision of contractor's work (maintenance/rectification/fault). Maintaining records for utilities,chilled water & condenser water usage/monthly work carried out/monthly fault call-out /outstanding works. Coordination/supervision of landlord on building related work (aircon,toilet, lighting,etc). Reporting of abnormality. 7 days/24 hours standby for any M & E related emergency call out. Monitoring of FMAS on Data Centre M & E facilities. Scheduling of FCUs and AHUs for aircon extension request. Obtain quotation for maintenance and improvement works from vendors/contractors. Carry out duties in accordance with QEHS policy, procedures and work instructions. Aware of the legal and other requirements and significant environment aspects/impacts (EAI) and occupational safety and health hazard/risks associated with their work activities.  Requirement:  Minimum qualification Diploma or equivalent. Minimum experience 2 years & above in relevant field. Tactful, analytical skills and responsible. Experience in Data Centre, M&E and facilities management is a must. Willing to standby for emergency call out.    Please include the following in your Resume Document * (*.DOC/PDF - Files should not exceed 2MB)    • Name    • Contact No.    • Nationality/PR Status    • Location/Address    • Recent Photo    • Expected Salary.","• Attractive salary packages • Company Bonuses, Benefits & Privileges • Career Progression Opportunities   Interested applicants can send your resume to supreme.cathrynteng@gmail.com and allow our Consultants to match you with our Clients. No Charges will be incurred by Candidates for any service rendered.   Role:  Daily health check non-critical/critical facilities & records. Attendance/initial troubleshooting to fault/breakdown/complaint & records. Attend user request for handyman works, eg. Cabinet door faulty, minor shifting etc and report to client CAS for follow up action. Generating the 1st incident report & escalation. Coordination/supervision of contractor's work (maintenance/rectification/fault). Maintaining records for utilities,chilled water & condenser water usage/monthly work carried out/monthly fault call-out /outstanding works. Coordination/supervision of landlord on building related work (aircon,toilet, lighting,etc). Reporting of abnormality. 7 days/24 hours standby for any M & E related emergency call out. Monitoring of FMAS on Data Centre M & E facilities. Scheduling of FCUs and AHUs for aircon extension request. Obtain quotation for maintenance and improvement works from vendors/contractors. Carry out duties in accordance with QEHS policy, procedures and work instructions. Aware of the legal and other requirements and significant environment aspects/impacts (EAI) and occupational safety and health hazard/risks associated with their work activities.  Requirement:  Minimum qualification Diploma or equivalent. Minimum experience 2 years & above in relevant field. Tactful, analytical skills and responsible. Experience in Data Centre, M&E and facilities management is a must. Willing to standby for emergency call out.    Please include the following in your Resume Document * (*.DOC/PDF - Files should not exceed 2MB)    • Name    • Contact No.    • Nationality/PR Status    • Location/Address    • Recent Photo    • Expected Salary."
126,ed592471e3bf926a6bcee241bc6dfe58,https://www.mycareersfuture.sg/job/devops-engineer-data-science-artificial-intelligence-government-technology-agency-ed592471e3bf926a6bcee241bc6dfe58,"DevOps Engineer, Data Science and Artificial Intelligence",Permanent,GOVERNMENT TECHNOLOGY AGENCY,"MAPLETREE BUSINESS CITY, 10 PASIR PANJANG ROAD 117438",Middle Management,"Information Technology, Public / Civil Service",5000,8000,,17 Dec 2018,0,0,0,1,"The Government Technology Agency (GovTech) aims to transform the delivery of Government digital services by taking an ""outside-in"" view, putting citizens and businesses at the heart of everything we do. We also develop the Smart Nation infrastructure and applications, and facilitate collaboration with citizens and businesses to co-develop technologies. Join us as we support Singapore’s vision of building a Smart Nation - a nation of possibilities empowered through info-communications technology and related engineering. Who we are: GovTech's Data Science and Artificial Intelligence team uses technology and data to help deliver high-quality digital services to citizens and businesses in Singapore. We build software for government agencies to better understand and use their data to improve operations and decision making. What the role is: You will work on both small and large scale projects, building and maintaining the infrastructure behind them. We are fully aligned with the Government’s cloud-first policy and you will bring capability to help us realise this. The role includes:  Managing the development, deployment, orchestration and maintenance of data pipelines for our Data Science products Providing DevOps architecture implementation and operational support Architecture and planning for cloud deployments (Private and Public cloud) Developing and managing processes, automation, best practices, documentation Development and operation of continuous integration and deployment pipelines.   What it is like working here: We build products that serve a variety of agency users, who use them to solve highly meaningful problems pertinent to our society, from transportation, to education, to healthcare. You are expected to have ownership over the problems that you solve. This means having ideas on how things should be done and taking responsibility for seeing them through. Building something that you believe in is the best way to build something good. As we often deal with big data and computing requirements, you are also able to take a long-term strategic view of the platforms you work on, and help provide this perspective to the team. To do so, you will:  Effectively prioritise and execute tasks in a high-pressure environment  Develop and maintain internal engineering productivity tools and environments Perform independent research into product and environment issues as required  Monitoring automation to effectively detect/predict/prevent issues in the environment and code base Future-proofing the technical environments and ensuring extreme high levels of automation, availability, scalability and resilience Hands-on coding and mentoring, working in highly collaborative teams and building quality environments Have knowledge in and/or continuously learn lots of different open source technologies and configurations  What we are looking for: The customers for our products are normally agency users, which means that breadth of knowledge in government IT infrastructure and experience in government networks will help. Since our direction is cloud-first, you will likely have some experience in patch/update scheduling, and knowledge of security incident response procedures. A disciplined approach and strong problem-solving instincts are fundamental to succeed. Your aptitude for completing the tasks and attitude to continuous learning are more valued than any formal certification. To succeed you will need to possess some of the following:  Excellent problem solving and methodical troubleshooting skills Strong knowledge and experience in DevOps automation, containerisation and orchestration using tools eg. Ansible, Airflow, Docker, Kubernetes, Terraform, Artifactory/Nexus Sonatype Cloud computing deployment and management experience in AWS, GCP Strong scripting skills e.g. Python, Bash, JavaScript, Scala, Rust, Go Strong understanding of Apache Spark/Flink, Hadoop, distributed file systems and resource scaling/scheduling, streaming message queues (RabbitMQ, Kafka) Strong understanding of virtualization and networking concepts Experience with patch maintenance, regression testing and security incident response Experience with interactive workloads, machine learning toolkits and how they integrate with cloud computing e.g. Databricks, KX Experience with highly scalable distributed systems Experience with on-premise deployments, government application and networking infrastructure/routing Breadth of knowledge - OS, networking, distributed computing, cloud computing ",None
